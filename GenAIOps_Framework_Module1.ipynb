{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPAfKeBuxHKpypo3gCkl2KP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rashmisingh100-dev/Project-X/blob/main/GenAIOps_Framework_Module1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFavuHwW2qXp",
        "outputId": "5f2a9898-aa4d-4366-b16c-4751ff30e8c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GenAIOps Framework - Starting Setup\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "#GenAI Ops Framework - Module 1: Foundation\n",
        "#This module builds the core GenAI components\n",
        "print(\"GenAIOps Framework - Starting Setup\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create Visual Directory Setup\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "Aaugcm203HlT"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Setup directories\n",
        "base_dir= Path('/content/genaiops')\n",
        "base_dir.mkdir(exist_ok=True)"
      ],
      "metadata": {
        "id": "XyDJvnhe3Csp"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create subsidirectories\n",
        "(base_dir/'prompts').mkdir(exist_ok=True)\n",
        "(base_dir / 'models').mkdir(exist_ok=True)\n",
        "(base_dir / 'evaluations').mkdir(exist_ok=True)\n",
        "(base_dir / 'logs').mkdir(exist_ok=True)"
      ],
      "metadata": {
        "id": "_-ISBxDc36Nj"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"‚úÖ Environment ready!\")\n",
        "print(f\"üìÅ Base directory: {base_dir}\")\n",
        "print()\n",
        "print(\"üëâ You can now run the rest of the notebook\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_7XCAwW3jLH",
        "outputId": "3641db93-627e-496b-bf82-05dfb7d79c3f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Environment ready!\n",
            "üìÅ Base directory: /content/genaiops\n",
            "\n",
            "üëâ You can now run the rest of the notebook\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#verify setup worked\n",
        "import os\n",
        "print(\"üîç Verifying GenAIOps directory structure...\")\n",
        "print()\n",
        "\n",
        "for folder in ['prompts', 'models', 'evaluations', 'logs']:\n",
        "    path = f'/content/genaiops/{folder}'\n",
        "    exists = os.path.exists(path)\n",
        "    status = \"‚úÖ\" if exists else \"‚ùå\"\n",
        "    print(f\"{status} {path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3ZmVuNXAXc-",
        "outputId": "c54aa0ff-5c9d-4a2b-aa50-6ce40d92c6c8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Verifying GenAIOps directory structure...\n",
            "\n",
            "‚úÖ /content/genaiops/prompts\n",
            "‚úÖ /content/genaiops/models\n",
            "‚úÖ /content/genaiops/evaluations\n",
            "‚úÖ /content/genaiops/logs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# COMPONENT 1: Prompt Management System\n",
        "# ========================================\n",
        "print(\"üìù Building Prompt Management System...\")\n",
        "print()\n",
        "#Define Prompt Template for Customer Support\n",
        "customer_support_prompt_v1 = \"\"\"\n",
        "You are a helpful customer service representative for Prudential Financial.\n",
        "\n",
        "Customer Question:{customer_question}\n",
        "\n",
        "Instructions:\n",
        "- Be professional and empathetic\n",
        "- Provide accurate information about policies, only factual and grounded answer with no hallucination\n",
        "- If you don't know the answer, say so clearly\n",
        "- Keep response under 150 words\n",
        "- Include next steps when applicable\n",
        "- Professional yet conversational tone\n",
        "- Include 2-3 specific next steps\n",
        "- Offer specialist escalation if complex\n",
        "\n",
        "Safety Rules:\n",
        "- Never provide medical advice\n",
        "- Never make financial predictions\n",
        "- Don't discuss other customers\n",
        "- Escalate legal questions to compliance team\n",
        "Response:\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpfeMYoEAvBE",
        "outputId": "530f8415-ce79-4a3c-ccad-8414b174df7b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìù Building Prompt Management System...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save this prompt to our prompts directory\n",
        "prompt_file_path = '/content/genaiops/prompts/customer_support_v1.0.txt'\n",
        "\n",
        "with open(prompt_file_path, 'w') as f:\n",
        "    f.write(customer_support_prompt_v1)\n",
        "\n",
        "print(f\"‚úÖ Prompt saved to: {prompt_file_path}\")\n",
        "print()\n",
        "print(\"üìÑ Prompt content:\")\n",
        "print(\"-\" * 50)\n",
        "print(customer_support_prompt_v1)\n"
      ],
      "metadata": {
        "id": "9cf-fKg8FgHE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae22dc18-ff42-484f-c34a-dbda9b2cb2e7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Prompt saved to: /content/genaiops/prompts/customer_support_v1.0.txt\n",
            "\n",
            "üìÑ Prompt content:\n",
            "--------------------------------------------------\n",
            "\n",
            "You are a helpful customer service representative for Prudential Financial.\n",
            "\n",
            "Customer Question:{customer_question}\n",
            "\n",
            "Instructions:\n",
            "- Be professional and empathetic\n",
            "- Provide accurate information about policies, only factual and grounded answer with no hallucination\n",
            "- If you don't know the answer, say so clearly\n",
            "- Keep response under 150 words\n",
            "- Include next steps when applicable\n",
            "- Professional yet conversational tone\n",
            "- Include 2-3 specific next steps\n",
            "- Offer specialist escalation if complex\n",
            "\n",
            "Safety Rules:\n",
            "- Never provide medical advice\n",
            "- Never make financial predictions\n",
            "- Don't discuss other customers\n",
            "- Escalate legal questions to compliance team\n",
            "Response:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prompt Metadata (Governance)\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# Create metadata for our prompt\n",
        "prompt_metadata = {\n",
        "    \"prompt_id\": \"customer_support_v1.0\",\n",
        "    \"version\": \"1.0\",\n",
        "    \"created_date\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
        "    \"created_by\": \"Rashmi Singh\",\n",
        "    \"status\": \"approved\",\n",
        "    \"use_case\": \"Customer service chatbot\",\n",
        "    \"model_compatibility\": [\"gemini-1.5-pro\", \"gemini-1.5-flash\"],\n",
        "    \"approved_by\": \"Data & AI COE (Group)\",\n",
        "    \"approval_date\": \"2024-02-14\",\n",
        "    \"description\": \"Professional customer service prompt with empathy and accuracy focus\",\n",
        "    \"test_pass_rate\": 0.95,  # 95% of test cases passed\n",
        "    \"production_apps\": [\"CustomerSupportBot\", \"EmailAutomation\"]\n",
        "}\n",
        "\n",
        "# Save metadata as JSON\n",
        "metadata_file = '/content/genaiops/prompts/customer_support_v1.0_metadata.json'\n",
        "\n",
        "with open(metadata_file, 'w') as f:\n",
        "    json.dump(prompt_metadata, f, indent=2)\n",
        "\n",
        "print(\"‚úÖ Prompt metadata saved\")\n",
        "print()\n",
        "print(\"üìã Metadata:\")\n",
        "print(json.dumps(prompt_metadata, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FLL4wkWmlYK",
        "outputId": "5a106463-347b-43a6-fd90-230024a70969"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Prompt metadata saved\n",
            "\n",
            "üìã Metadata:\n",
            "{\n",
            "  \"prompt_id\": \"customer_support_v1.0\",\n",
            "  \"version\": \"1.0\",\n",
            "  \"created_date\": \"2026-02-15\",\n",
            "  \"created_by\": \"Rashmi Singh\",\n",
            "  \"status\": \"approved\",\n",
            "  \"use_case\": \"Customer service chatbot\",\n",
            "  \"model_compatibility\": [\n",
            "    \"gemini-1.5-pro\",\n",
            "    \"gemini-1.5-flash\"\n",
            "  ],\n",
            "  \"approved_by\": \"Data & AI COE (Group)\",\n",
            "  \"approval_date\": \"2024-02-14\",\n",
            "  \"description\": \"Professional customer service prompt with empathy and accuracy focus\",\n",
            "  \"test_pass_rate\": 0.95,\n",
            "  \"production_apps\": [\n",
            "    \"CustomerSupportBot\",\n",
            "    \"EmailAutomation\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prompt Loader Function\n",
        "\n",
        "def load_prompt(prompt_id, version=\"latest\"):\n",
        "    \"\"\"\n",
        "    Load a prompt template by ID and version\n",
        "\n",
        "    Args:\n",
        "        prompt_id: Name of the prompt (e.g., 'customer_support')\n",
        "        version: Version number (e.g., '1.0') or 'latest'\n",
        "\n",
        "    Returns:\n",
        "        dict with 'template' and 'metadata'\n",
        "    \"\"\"\n",
        "\n",
        "    # Construct file paths\n",
        "    if version == \"latest\":\n",
        "        # In real system, would query database for latest version\n",
        "        # For now, we'll use v1.0\n",
        "        version = \"1.0\"\n",
        "\n",
        "    prompt_file = f'/content/genaiops/prompts/{prompt_id}_v{version}.txt'\n",
        "    metadata_file = f'/content/genaiops/prompts/{prompt_id}_v{version}_metadata.json'\n",
        "\n",
        "    # Load prompt template\n",
        "    try:\n",
        "        with open(prompt_file, 'r') as f:\n",
        "            template = f.read()\n",
        "    except FileNotFoundError:\n",
        "        return {\"error\": f\"Prompt {prompt_id} v{version} not found\"}\n",
        "\n",
        "    # Load metadata\n",
        "    try:\n",
        "        with open(metadata_file, 'r') as f:\n",
        "            metadata = json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        metadata = {\"warning\": \"No metadata found\"}\n",
        "\n",
        "    return {\n",
        "        \"template\": template,\n",
        "        \"metadata\": metadata\n",
        "    }\n",
        "\n",
        "\n",
        "# Test the loader\n",
        "print(\"üß™ Testing prompt loader...\")\n",
        "print()\n",
        "\n",
        "result = load_prompt(\"customer_support\", version=\"1.0\")\n",
        "\n",
        "print(\"‚úÖ Prompt loaded successfully!\")\n",
        "print()\n",
        "print(\"üìÑ Template:\")\n",
        "print(result['template'][:200] + \"...\")  # First 200 chars\n",
        "print()\n",
        "print(\"üìã Metadata:\")\n",
        "print(f\"  Version: {result['metadata']['version']}\")\n",
        "print(f\"  Status: {result['metadata']['status']}\")\n",
        "print(f\"  Use Case: {result['metadata']['use_case']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4w5KD_4xq2Wf",
        "outputId": "25fa2d02-9317-4de7-f01a-70b768048160"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ Testing prompt loader...\n",
            "\n",
            "‚úÖ Prompt loaded successfully!\n",
            "\n",
            "üìÑ Template:\n",
            "\n",
            "You are a helpful customer service representative for Prudential Financial.\n",
            "\n",
            "Customer Question:{customer_question}\n",
            "\n",
            "Instructions:\n",
            "- Be professional and empathetic\n",
            "- Provide accurate information about...\n",
            "\n",
            "üìã Metadata:\n",
            "  Version: 1.0\n",
            "  Status: approved\n",
            "  Use Case: Customer service chatbot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prompt Version Comparison Tool\n",
        "# Create an improved version (v1.1)\n",
        "customer_support_prompt_v1_1 = \"\"\"\n",
        "You are an empathetic customer service representative for Prudential Financial with deep knowledge of our insurance products and policies.\n",
        "\n",
        "Customer Profile:\n",
        "- Name: {customer_name}\n",
        "- Policy Type: {policy_type}\n",
        "- Customer Since: {customer_since}\n",
        "\n",
        "Customer Question:\n",
        "{customer_question}\n",
        "\n",
        "Instructions:\n",
        "- Address customer by name to personalize the response\n",
        "- Be professional, empathetic, and solution-oriented\n",
        "- Reference their specific policy type when relevant\n",
        "- Provide accurate information about Prudential policies\n",
        "- If you don't know the answer, be honest and offer to connect them with a specialist\n",
        "- Keep response under 150 words\n",
        "- Always include clear next steps\n",
        "- End with \"Is there anything else I can help you with today?\"\n",
        "- Professional yet conversational tone\n",
        "- Include 2-3 specific next steps\n",
        "- Offer specialist escalation if complex\n",
        "\n",
        "Safety Rules:\n",
        "- Never provide medical advice\n",
        "- Never make financial predictions\n",
        "- Don't discuss other customers\n",
        "- Escalate legal questions to compliance team\n",
        "\n",
        "Response:\n",
        "\"\"\"\n",
        "\n",
        "# Save v1.1\n",
        "prompt_v1_1_path = '/content/genaiops/prompts/customer_support_v1.1.txt'\n",
        "with open(prompt_v1_1_path, 'w') as f:\n",
        "    f.write(customer_support_prompt_v1_1)\n",
        "\n",
        "# Create metadata for v1.1\n",
        "metadata_v1_1 = {\n",
        "    \"prompt_id\": \"customer_support_v1.1\",\n",
        "    \"version\": \"1.1\",\n",
        "    \"created_date\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
        "    \"created_by\": \"Rashmi Singh\",\n",
        "    \"status\": \"testing\",  # Not yet approved for production\n",
        "    \"use_case\": \"Customer service chatbot\",\n",
        "    \"model_compatibility\": [\"gemini-1.5-pro\", \"gemini-1.5-flash\"],\n",
        "    \"description\": \"Enhanced with personalization and policy-type awareness\",\n",
        "    \"improvements_over_v1.0\": [\n",
        "        \"Personalization with customer name\",\n",
        "        \"Policy-type specific responses\",\n",
        "        \"Customer tenure awareness\",\n",
        "        \"Standardized closing question\"\n",
        "    ],\n",
        "    \"test_pass_rate\": None,  # Not yet tested\n",
        "    \"production_apps\": []  # Not yet deployed\n",
        "}\n",
        "\n",
        "metadata_v1_1_path = '/content/genaiops/prompts/customer_support_v1.1_metadata.json'\n",
        "with open(metadata_v1_1_path, 'w') as f:\n",
        "    json.dump(metadata_v1_1, f, indent=2)\n",
        "\n",
        "print(\"‚úÖ Created prompt v1.1 (improved version)\")\n",
        "print()\n",
        "print(\"üÜö Comparing v1.0 vs v1.1:\")\n",
        "print(\"-\" * 60)\n",
        "print(\"v1.0 (Production):\")\n",
        "print(\"  - Generic customer addressing\")\n",
        "print(\"  - No personalization\")\n",
        "print(\"  - Status: Approved ‚úÖ\")\n",
        "print()\n",
        "print(\"v1.1 (Testing):\")\n",
        "print(\"  - Personalized with customer name\")\n",
        "print(\"  - Policy-type aware\")\n",
        "print(\"  - Customer tenure aware\")\n",
        "print(\"  - Standardized closing\")\n",
        "print(\"  - Status: Testing üß™\")\n",
        "print()\n",
        "print(\"üìä Next step: A/B testing to compare quality\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NWdsm_grCZO",
        "outputId": "a6a7432a-c5dc-4e9d-a79f-63671ece23e1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Created prompt v1.1 (improved version)\n",
            "\n",
            "üÜö Comparing v1.0 vs v1.1:\n",
            "------------------------------------------------------------\n",
            "v1.0 (Production):\n",
            "  - Generic customer addressing\n",
            "  - No personalization\n",
            "  - Status: Approved ‚úÖ\n",
            "\n",
            "v1.1 (Testing):\n",
            "  - Personalized with customer name\n",
            "  - Policy-type aware\n",
            "  - Customer tenure aware\n",
            "  - Standardized closing\n",
            "  - Status: Testing üß™\n",
            "\n",
            "üìä Next step: A/B testing to compare quality\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# FEATURE 1: A/B Testing Framework\n",
        "# ========================================\n",
        "\n",
        "print(\"üî¨ Building A/B Testing Framework for Prompts...\")\n",
        "print()\n",
        "\n",
        "import hashlib\n",
        "\n",
        "class ABTestManager:\n",
        "    \"\"\"\n",
        "    Manages A/B tests for prompt versions\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.active_tests = {}\n",
        "        self.test_results = {}\n",
        "\n",
        "    def create_ab_test(self, test_id, prompt_id, variant_a_version,\n",
        "                       variant_b_version, traffic_split=0.5):\n",
        "        \"\"\"\n",
        "        Create a new A/B test\n",
        "\n",
        "        Args:\n",
        "            test_id: Unique test identifier\n",
        "            prompt_id: Which prompt to test\n",
        "            variant_a_version: Control version (e.g., \"1.0\")\n",
        "            variant_b_version: Treatment version (e.g., \"1.1\")\n",
        "            traffic_split: % of traffic to variant B (0.0 to 1.0)\n",
        "        \"\"\"\n",
        "\n",
        "        self.active_tests[test_id] = {\n",
        "            \"test_id\": test_id,\n",
        "            \"prompt_id\": prompt_id,\n",
        "            \"variant_a\": {\n",
        "                \"version\": variant_a_version,\n",
        "                \"traffic\": 1 - traffic_split,\n",
        "                \"requests\": 0,\n",
        "                \"label\": \"Control (A)\"\n",
        "            },\n",
        "            \"variant_b\": {\n",
        "                \"version\": variant_b_version,\n",
        "                \"traffic\": traffic_split,\n",
        "                \"requests\": 0,\n",
        "                \"label\": \"Treatment (B)\"\n",
        "            },\n",
        "            \"status\": \"active\",\n",
        "            \"created_date\": \"2024-02-14\",\n",
        "            \"total_requests\": 0\n",
        "        }\n",
        "\n",
        "        # Save test configuration\n",
        "        test_file = f'/content/genaiops/prompts/ab_test_{test_id}.json'\n",
        "        with open(test_file, 'w') as f:\n",
        "            json.dump(self.active_tests[test_id], f, indent=2)\n",
        "\n",
        "        print(f\"‚úÖ A/B Test Created: {test_id}\")\n",
        "        print(f\"   Prompt: {prompt_id}\")\n",
        "        print(f\"   Variant A (Control): v{variant_a_version} - {(1-traffic_split)*100:.0f}% traffic\")\n",
        "        print(f\"   Variant B (Treatment): v{variant_b_version} - {traffic_split*100:.0f}% traffic\")\n",
        "\n",
        "        return self.active_tests[test_id]\n",
        "\n",
        "    def assign_variant(self, test_id, user_id):\n",
        "        \"\"\"\n",
        "        Assign a user to variant A or B\n",
        "        Uses consistent hashing so same user always gets same variant\n",
        "\n",
        "        Args:\n",
        "            test_id: Which test\n",
        "            user_id: User identifier (email, customer ID, etc.)\n",
        "\n",
        "        Returns:\n",
        "            dict with assigned variant info\n",
        "        \"\"\"\n",
        "\n",
        "        if test_id not in self.active_tests:\n",
        "            return {\"error\": f\"Test {test_id} not found\"}\n",
        "\n",
        "        test = self.active_tests[test_id]\n",
        "\n",
        "        # Consistent hashing: same user_id always gets same variant\n",
        "        hash_input = f\"{test_id}:{user_id}\".encode()\n",
        "        hash_value = int(hashlib.md5(hash_input).hexdigest(), 16)\n",
        "        user_hash = (hash_value % 100) / 100  # 0.00 to 0.99\n",
        "\n",
        "        # Assign variant based on traffic split\n",
        "        if user_hash < test[\"variant_b\"][\"traffic\"]:\n",
        "            assigned_variant = \"B\"\n",
        "            version = test[\"variant_b\"][\"version\"]\n",
        "            test[\"variant_b\"][\"requests\"] += 1\n",
        "        else:\n",
        "            assigned_variant = \"A\"\n",
        "            version = test[\"variant_a\"][\"version\"]\n",
        "            test[\"variant_a\"][\"requests\"] += 1\n",
        "\n",
        "        test[\"total_requests\"] += 1\n",
        "\n",
        "        return {\n",
        "            \"test_id\": test_id,\n",
        "            \"user_id\": user_id,\n",
        "            \"assigned_variant\": assigned_variant,\n",
        "            \"prompt_version\": version,\n",
        "            \"prompt_id\": test[\"prompt_id\"]\n",
        "        }\n",
        "\n",
        "    def get_prompt_for_user(self, test_id, user_id):\n",
        "        \"\"\"\n",
        "        Get the appropriate prompt version for a user in an A/B test\n",
        "\n",
        "        Args:\n",
        "            test_id: Which test\n",
        "            user_id: User identifier\n",
        "\n",
        "        Returns:\n",
        "            Prompt template for the assigned variant\n",
        "        \"\"\"\n",
        "\n",
        "        # Assign variant\n",
        "        assignment = self.assign_variant(test_id, user_id)\n",
        "\n",
        "        if \"error\" in assignment:\n",
        "            return assignment\n",
        "\n",
        "        # Load the appropriate prompt version\n",
        "        prompt_id = assignment[\"prompt_id\"]\n",
        "        version = assignment[\"prompt_version\"]\n",
        "\n",
        "        prompt_data = load_prompt(prompt_id, version)\n",
        "\n",
        "        return {\n",
        "            \"assignment\": assignment,\n",
        "            \"prompt\": prompt_data\n",
        "        }\n",
        "\n",
        "    def get_test_stats(self, test_id):\n",
        "        \"\"\"\n",
        "        Get statistics for an A/B test\n",
        "        \"\"\"\n",
        "\n",
        "        if test_id not in self.active_tests:\n",
        "            return {\"error\": f\"Test {test_id} not found\"}\n",
        "\n",
        "        test = self.active_tests[test_id]\n",
        "\n",
        "        return {\n",
        "            \"test_id\": test_id,\n",
        "            \"status\": test[\"status\"],\n",
        "            \"total_requests\": test[\"total_requests\"],\n",
        "            \"variant_a\": {\n",
        "                \"version\": test[\"variant_a\"][\"version\"],\n",
        "                \"requests\": test[\"variant_a\"][\"requests\"],\n",
        "                \"percentage\": (test[\"variant_a\"][\"requests\"] / test[\"total_requests\"] * 100)\n",
        "                              if test[\"total_requests\"] > 0 else 0\n",
        "            },\n",
        "            \"variant_b\": {\n",
        "                \"version\": test[\"variant_b\"][\"version\"],\n",
        "                \"requests\": test[\"variant_b\"][\"requests\"],\n",
        "                \"percentage\": (test[\"variant_b\"][\"requests\"] / test[\"total_requests\"] * 100)\n",
        "                              if test[\"total_requests\"] > 0 else 0\n",
        "            }\n",
        "        }\n",
        "\n",
        "\n",
        "# ========================================\n",
        "# Test the A/B Testing Framework\n",
        "# ========================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üß™ Testing A/B Framework...\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "# Create A/B test manager\n",
        "ab_manager = ABTestManager()\n",
        "\n",
        "# Create a test: 80% get v1.0, 20% get v1.1\n",
        "test = ab_manager.create_ab_test(\n",
        "    test_id=\"customer_support_feb_2024\",\n",
        "    prompt_id=\"customer_support\",\n",
        "    variant_a_version=\"1.0\",  # Control (80%)\n",
        "    variant_b_version=\"1.1\",  # Treatment (20%)\n",
        "    traffic_split=0.2         # 20% to variant B\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 70)\n",
        "print(\"üìä Simulating 100 User Requests...\")\n",
        "print(\"-\" * 70 + \"\\n\")\n",
        "\n",
        "# Simulate 100 users\n",
        "for i in range(100):\n",
        "    user_id = f\"user_{i}@prudential.com\"\n",
        "    result = ab_manager.get_prompt_for_user(\"customer_support_feb_2024\", user_id)\n",
        "\n",
        "# Get statistics\n",
        "stats = ab_manager.get_test_stats(\"customer_support_feb_2024\")\n",
        "\n",
        "print(\"üìà A/B Test Results:\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"Test ID: {stats['test_id']}\")\n",
        "print(f\"Status: {stats['status']}\")\n",
        "print(f\"Total Requests: {stats['total_requests']}\")\n",
        "print()\n",
        "print(f\"Variant A (Control - v{stats['variant_a']['version']}):\")\n",
        "print(f\"  Requests: {stats['variant_a']['requests']}\")\n",
        "print(f\"  Percentage: {stats['variant_a']['percentage']:.1f}%\")\n",
        "print()\n",
        "print(f\"Variant B (Treatment - v{stats['variant_b']['version']}):\")\n",
        "print(f\"  Requests: {stats['variant_b']['requests']}\")\n",
        "print(f\"  Percentage: {stats['variant_b']['percentage']:.1f}%\")\n",
        "print()\n",
        "\n",
        "# Demonstrate consistent hashing\n",
        "print(\"-\" * 70)\n",
        "print(\"üîí Testing Consistent Hashing (same user = same variant)...\")\n",
        "print(\"-\" * 70 + \"\\n\")\n",
        "\n",
        "test_user = \"alice@prudential.com\"\n",
        "assignments = []\n",
        "\n",
        "for i in range(5):\n",
        "    result = ab_manager.assign_variant(\"customer_support_feb_2024\", test_user)\n",
        "    assignments.append(result[\"assigned_variant\"])\n",
        "\n",
        "print(f\"User: {test_user}\")\n",
        "print(f\"Assignment (5 requests): {assignments}\")\n",
        "print(f\"‚úÖ All same variant: {len(set(assignments)) == 1}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ A/B Testing Framework Complete!\")\n",
        "print(\"=\" * 70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jR3sM8JlbAC_",
        "outputId": "167d486e-9402-42a2-8b8f-27499be16e28"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üî¨ Building A/B Testing Framework for Prompts...\n",
            "\n",
            "\n",
            "======================================================================\n",
            "üß™ Testing A/B Framework...\n",
            "======================================================================\n",
            "\n",
            "‚úÖ A/B Test Created: customer_support_feb_2024\n",
            "   Prompt: customer_support\n",
            "   Variant A (Control): v1.0 - 80% traffic\n",
            "   Variant B (Treatment): v1.1 - 20% traffic\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "üìä Simulating 100 User Requests...\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "üìà A/B Test Results:\n",
            "----------------------------------------------------------------------\n",
            "Test ID: customer_support_feb_2024\n",
            "Status: active\n",
            "Total Requests: 100\n",
            "\n",
            "Variant A (Control - v1.0):\n",
            "  Requests: 82\n",
            "  Percentage: 82.0%\n",
            "\n",
            "Variant B (Treatment - v1.1):\n",
            "  Requests: 18\n",
            "  Percentage: 18.0%\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "üîí Testing Consistent Hashing (same user = same variant)...\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "User: alice@prudential.com\n",
            "Assignment (5 requests): ['A', 'A', 'A', 'A', 'A']\n",
            "‚úÖ All same variant: True\n",
            "\n",
            "======================================================================\n",
            "‚úÖ A/B Testing Framework Complete!\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# COMPONENT 2: Model Registry (CORE - NO ASSUMPTIONS)\n",
        "# ========================================\n",
        "\n",
        "from datetime import datetime\n",
        "import json\n",
        "\n",
        "print(\"ü§ñ Building Model Registry...\")\n",
        "print()\n",
        "\n",
        "# Define our approved model catalog\n",
        "model_catalog = {\n",
        "    \"gemini-1.5-pro\": {\n",
        "        \"model_id\": \"gemini-1.5-pro\",\n",
        "        \"display_name\": \"Gemini 1.5 Pro\",\n",
        "        \"provider\": \"Google\",\n",
        "        \"model_type\": \"foundation\",\n",
        "        \"status\": \"approved\",\n",
        "        \"tier\": \"premium\",\n",
        "        \"capabilities\": [\"text-generation\", \"code-generation\", \"analysis\"],\n",
        "        \"max_tokens\": 2000000,\n",
        "\n",
        "        # Null until actually set\n",
        "        \"created_date\": None,\n",
        "        \"approved_date\": None,\n",
        "        \"approved_by\": None,\n",
        "        \"training_data\": None\n",
        "    },\n",
        "\n",
        "    \"gemini-1.5-flash\": {\n",
        "        \"model_id\": \"gemini-1.5-flash\",\n",
        "        \"display_name\": \"Gemini 1.5 Flash\",\n",
        "        \"provider\": \"Google\",\n",
        "        \"model_type\": \"foundation\",\n",
        "        \"status\": \"approved\",\n",
        "        \"tier\": \"standard\",\n",
        "        \"capabilities\": [\"text-generation\", \"high-volume-tasks\"],\n",
        "        \"max_tokens\": 1000000,\n",
        "\n",
        "        # Null until actually set\n",
        "        \"created_date\": None,\n",
        "        \"approved_date\": None,\n",
        "        \"approved_by\": None,\n",
        "        \"training_data\": None\n",
        "    },\n",
        "\n",
        "    \"claude-3-opus\": {\n",
        "        \"model_id\": \"claude-3-opus\",\n",
        "        \"display_name\": \"Claude 3 Opus\",\n",
        "        \"provider\": \"Anthropic\",\n",
        "        \"model_type\": \"foundation\",\n",
        "        \"status\": \"approved\",\n",
        "        \"tier\": \"premium\",\n",
        "        \"capabilities\": [\"text-generation\", \"analysis\", \"long-context\"],\n",
        "        \"max_tokens\": 200000,\n",
        "\n",
        "        # Null until actually set\n",
        "        \"created_date\": None,\n",
        "        \"approved_date\": None,\n",
        "        \"approved_by\": None,\n",
        "        \"training_data\": None\n",
        "    },\n",
        "\n",
        "    \"customer-support-v1.2\": {\n",
        "        \"model_id\": \"customer-support-v1.2\",\n",
        "        \"display_name\": \"Customer Support Model v1.2\",\n",
        "        \"provider\": \"Prudential (Fine-tuned Gemini)\",\n",
        "        \"model_type\": \"fine-tuned\",\n",
        "        \"base_model\": \"gemini-1.5-pro\",\n",
        "        \"status\": \"approved\",\n",
        "        \"tier\": \"custom\",\n",
        "        \"capabilities\": [\"customer-service\", \"policy-questions\"],\n",
        "        \"max_tokens\": 2000000,\n",
        "        \"use_case_restriction\": \"customer-support-only\",\n",
        "\n",
        "        # Null until actually set\n",
        "        \"created_date\": None,\n",
        "        \"approved_date\": None,\n",
        "        \"approved_by\": None,\n",
        "        \"training_data\": None,\n",
        "        \"training_date\": None,\n",
        "        \"training_job_id\": None\n",
        "    },\n",
        "\n",
        "    \"gpt-4\": {\n",
        "        \"model_id\": \"gpt-4\",\n",
        "        \"display_name\": \"GPT-4\",\n",
        "        \"provider\": \"OpenAI\",\n",
        "        \"model_type\": \"foundation\",\n",
        "        \"status\": \"deprecated\",\n",
        "        \"tier\": \"premium\",\n",
        "        \"capabilities\": [\"text-generation\"],\n",
        "        \"max_tokens\": 128000,\n",
        "        \"replacement_model\": \"gemini-1.5-pro\",\n",
        "\n",
        "        # Null until actually set\n",
        "        \"created_date\": None,\n",
        "        \"deprecated_date\": None,\n",
        "        \"deprecated_by\": None,\n",
        "        \"deprecated_reason\": None,\n",
        "        \"training_data\": None\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save catalog to file\n",
        "catalog_file = '/content/genaiops/models/model_catalog.json'\n",
        "with open(catalog_file, 'w') as f:\n",
        "    json.dump(model_catalog, f, indent=2)\n",
        "\n",
        "print(f\"‚úÖ Model catalog created with {len(model_catalog)} models\")\n",
        "print(f\"   Saved to: {catalog_file}\")\n",
        "print()\n",
        "\n",
        "# Display summary\n",
        "print(\"üìä Model Summary:\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"{'Model':<35} {'Status':<12} {'Type':<15} {'Provider':<20}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for model_id, details in model_catalog.items():\n",
        "    status_emoji = \"‚úÖ\" if details[\"status\"] == \"approved\" else \"‚ö†Ô∏è\" if details[\"status\"] == \"testing\" else \"‚ùå\"\n",
        "    model_name = details['display_name']\n",
        "    status = details['status']\n",
        "    model_type = details['model_type']\n",
        "    provider = details['provider']\n",
        "\n",
        "    print(f\"{status_emoji} {model_name:<33} {status:<12} {model_type:<15} {provider:<20}\")\n",
        "\n",
        "print()\n",
        "print(\"=\" * 70)\n",
        "print(\"‚úÖ Component 2: Model Registry - CORE COMPLETE\")\n",
        "print(\"=\" * 70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMKMOvCduggL",
        "outputId": "38834412-18fa-4591-bb1d-2f5cc15028f4"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü§ñ Building Model Registry...\n",
            "\n",
            "‚úÖ Model catalog created with 5 models\n",
            "   Saved to: /content/genaiops/models/model_catalog.json\n",
            "\n",
            "üìä Model Summary:\n",
            "----------------------------------------------------------------------\n",
            "Model                               Status       Type            Provider            \n",
            "----------------------------------------------------------------------\n",
            "‚úÖ Gemini 1.5 Pro                    approved     foundation      Google              \n",
            "‚úÖ Gemini 1.5 Flash                  approved     foundation      Google              \n",
            "‚úÖ Claude 3 Opus                     approved     foundation      Anthropic           \n",
            "‚úÖ Customer Support Model v1.2       approved     fine-tuned      Prudential (Fine-tuned Gemini)\n",
            "‚ùå GPT-4                             deprecated   foundation      OpenAI              \n",
            "\n",
            "======================================================================\n",
            "‚úÖ Component 2: Model Registry - CORE COMPLETE\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# Add Cost Information to Model Registry\n",
        "# ========================================\n",
        "\n",
        "print(\"üí∞ Adding cost tracking to Model Registry...\")\n",
        "print()\n",
        "\n",
        "# Cost data for each model (pricing per 1K tokens)\n",
        "model_costs = {\n",
        "    \"gemini-1.5-pro\": {\n",
        "        \"input_cost_per_1k\": 0.00125,   # $0.00125 per 1K input tokens\n",
        "        \"output_cost_per_1k\": 0.005,    # $0.005 per 1K output tokens\n",
        "        \"cost_tier\": \"premium\",\n",
        "        \"notes\": \"Best quality, highest cost\"\n",
        "    },\n",
        "\n",
        "    \"gemini-1.5-flash\": {\n",
        "        \"input_cost_per_1k\": 0.000075,  # $0.000075 per 1K input tokens\n",
        "        \"output_cost_per_1k\": 0.0003,   # $0.0003 per 1K output tokens\n",
        "        \"cost_tier\": \"budget\",\n",
        "        \"notes\": \"Fast and cheap, good for high-volume\"\n",
        "    },\n",
        "\n",
        "    \"claude-3-opus\": {\n",
        "        \"input_cost_per_1k\": 0.015,     # $0.015 per 1K input tokens\n",
        "        \"output_cost_per_1k\": 0.075,    # $0.075 per 1K output tokens\n",
        "        \"cost_tier\": \"premium-plus\",\n",
        "        \"notes\": \"Most expensive, best for complex tasks\"\n",
        "    },\n",
        "\n",
        "    \"customer-support-v1.2\": {\n",
        "        \"input_cost_per_1k\": 0.00125,   # Same as base Gemini Pro\n",
        "        \"output_cost_per_1k\": 0.005,\n",
        "        \"cost_tier\": \"premium\",\n",
        "        \"training_cost\": 850.00,        # One-time training cost\n",
        "        \"notes\": \"Fine-tuned model, training cost already paid\"\n",
        "    },\n",
        "\n",
        "    \"gpt-4\": {\n",
        "        \"input_cost_per_1k\": 0.03,      # Expensive (deprecated)\n",
        "        \"output_cost_per_1k\": 0.06,\n",
        "        \"cost_tier\": \"deprecated\",\n",
        "        \"notes\": \"Deprecated - do not use\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save cost data\n",
        "cost_file = '/content/genaiops/models/model_costs.json'\n",
        "with open(cost_file, 'w') as f:\n",
        "    json.dump(model_costs, f, indent=2)\n",
        "\n",
        "print(\"‚úÖ Cost tracking added for all models\")\n",
        "print()\n",
        "\n",
        "# Display cost comparison\n",
        "print(\"üíµ Cost Comparison (per 1K tokens):\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"{'Model':<30} {'Input Cost':>12} {'Output Cost':>12} {'Tier':<15}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for model_id, costs in model_costs.items():\n",
        "    if model_id in model_catalog and model_catalog[model_id][\"status\"] != \"deprecated\":\n",
        "        input_cost = f\"${costs['input_cost_per_1k']:.6f}\"\n",
        "        output_cost = f\"${costs['output_cost_per_1k']:.6f}\"\n",
        "        tier = costs['cost_tier']\n",
        "\n",
        "        model_name = model_catalog[model_id][\"display_name\"]\n",
        "        print(f\"{model_name:<30} {input_cost:>12} {output_cost:>12} {tier:<15}\")\n",
        "\n",
        "print()\n",
        "\n",
        "# Calculate example: 1 million tokens\n",
        "print(\"üìä Example Cost Calculation:\")\n",
        "print(\"Scenario: Process 1M input tokens + generate 500K output tokens\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "example_input_tokens = 1000000  # 1M tokens\n",
        "example_output_tokens = 500000  # 500K tokens\n",
        "\n",
        "for model_id in [\"gemini-1.5-pro\", \"gemini-1.5-flash\", \"claude-3-opus\"]:\n",
        "    if model_id in model_costs:\n",
        "        costs = model_costs[model_id]\n",
        "\n",
        "        # Calculate cost\n",
        "        input_cost = (example_input_tokens / 1000) * costs['input_cost_per_1k']\n",
        "        output_cost = (example_output_tokens / 1000) * costs['output_cost_per_1k']\n",
        "        total_cost = input_cost + output_cost\n",
        "\n",
        "        model_name = model_catalog[model_id][\"display_name\"]\n",
        "        print(f\"{model_name:<30} Total Cost: ${total_cost:,.2f}\")\n",
        "\n",
        "print()\n",
        "print(\"üí° Insight: Gemini Flash is 94% cheaper than Claude Opus for high-volume tasks!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVg_Xcw4IiMI",
        "outputId": "cacab482-3c11-4d36-d297-851d822ca808"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üí∞ Adding cost tracking to Model Registry...\n",
            "\n",
            "‚úÖ Cost tracking added for all models\n",
            "\n",
            "üíµ Cost Comparison (per 1K tokens):\n",
            "--------------------------------------------------------------------------------\n",
            "Model                            Input Cost  Output Cost Tier           \n",
            "--------------------------------------------------------------------------------\n",
            "Gemini 1.5 Pro                    $0.001250    $0.005000 premium        \n",
            "Gemini 1.5 Flash                  $0.000075    $0.000300 budget         \n",
            "Claude 3 Opus                     $0.015000    $0.075000 premium-plus   \n",
            "Customer Support Model v1.2       $0.001250    $0.005000 premium        \n",
            "\n",
            "üìä Example Cost Calculation:\n",
            "Scenario: Process 1M input tokens + generate 500K output tokens\n",
            "--------------------------------------------------------------------------------\n",
            "Gemini 1.5 Pro                 Total Cost: $3.75\n",
            "Gemini 1.5 Flash               Total Cost: $0.22\n",
            "Claude 3 Opus                  Total Cost: $52.50\n",
            "\n",
            "üí° Insight: Gemini Flash is 94% cheaper than Claude Opus for high-volume tasks!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# Model Loader Function\n",
        "# ========================================\n",
        "\n",
        "def load_model_info(model_id):\n",
        "    \"\"\"\n",
        "    Load model information from registry\n",
        "\n",
        "    Args:\n",
        "        model_id: ID of the model (e.g., 'gemini-1.5-pro')\n",
        "\n",
        "    Returns:\n",
        "        dict with model details, costs, and approval status\n",
        "    \"\"\"\n",
        "\n",
        "    # Check if model exists in catalog\n",
        "    if model_id not in model_catalog:\n",
        "        return {\n",
        "            \"error\": f\"Model '{model_id}' not found in registry\",\n",
        "            \"available_models\": list(model_catalog.keys())\n",
        "        }\n",
        "\n",
        "    # Get model details\n",
        "    model_details = model_catalog[model_id]\n",
        "\n",
        "    # Check if model is approved\n",
        "    if model_details[\"status\"] == \"deprecated\":\n",
        "        return {\n",
        "            \"error\": f\"Model '{model_id}' is deprecated\",\n",
        "            \"status\": \"deprecated\",\n",
        "            \"deprecated_reason\": model_details.get(\"deprecated_reason\", \"Unknown\"),\n",
        "            \"replacement\": model_details.get(\"replacement_model\", \"Contact ML team\")\n",
        "        }\n",
        "\n",
        "    if model_details[\"status\"] != \"approved\":\n",
        "        return {\n",
        "            \"error\": f\"Model '{model_id}' is not approved for use\",\n",
        "            \"status\": model_details[\"status\"],\n",
        "            \"message\": \"Only approved models can be used in production\"\n",
        "        }\n",
        "\n",
        "    # Get cost information\n",
        "    cost_info = model_costs.get(model_id, {\n",
        "        \"input_cost_per_1k\": \"Unknown\",\n",
        "        \"output_cost_per_1k\": \"Unknown\",\n",
        "        \"cost_tier\": \"Unknown\"\n",
        "    })\n",
        "\n",
        "    # Combine all information\n",
        "    return {\n",
        "        \"model_id\": model_id,\n",
        "        \"details\": model_details,\n",
        "        \"costs\": cost_info,\n",
        "        \"status\": \"ready\",\n",
        "        \"message\": f\"‚úÖ {model_details['display_name']} is approved and ready to use\"\n",
        "    }\n",
        "\n",
        "\n",
        "# ========================================\n",
        "# Test the Model Loader\n",
        "# ========================================\n",
        "\n",
        "print(\"üß™ Testing Model Loader Function...\")\n",
        "print()\n",
        "\n",
        "# Test 1: Load approved model\n",
        "print(\"Test 1: Load Gemini 1.5 Flash (approved)\")\n",
        "print(\"-\" * 60)\n",
        "result1 = load_model_info(\"gemini-1.5-flash\")\n",
        "\n",
        "if \"error\" not in result1:\n",
        "    print(f\"‚úÖ {result1['message']}\")\n",
        "    print(f\"   Provider: {result1['details']['provider']}\")\n",
        "    print(f\"   Tier: {result1['details']['tier']}\")\n",
        "    print(f\"   Input Cost: ${result1['costs']['input_cost_per_1k']:.6f} per 1K tokens\")\n",
        "    print(f\"   Output Cost: ${result1['costs']['output_cost_per_1k']:.6f} per 1K tokens\")\n",
        "else:\n",
        "    print(f\"‚ùå {result1['error']}\")\n",
        "\n",
        "print()\n",
        "\n",
        "# Test 2: Try to load deprecated model\n",
        "print(\"Test 2: Try to load GPT-4 (deprecated)\")\n",
        "print(\"-\" * 60)\n",
        "result2 = load_model_info(\"gpt-4\")\n",
        "\n",
        "if \"error\" in result2:\n",
        "    print(f\"‚ùå {result2['error']}\")\n",
        "    print(f\"   Reason: {result2.get('deprecated_reason', 'N/A')}\")\n",
        "    print(f\"   Use instead: {result2.get('replacement', 'N/A')}\")\n",
        "else:\n",
        "    print(f\"‚úÖ Model loaded\")\n",
        "\n",
        "print()\n",
        "\n",
        "# Test 3: Try to load non-existent model\n",
        "print(\"Test 3: Try to load non-existent model\")\n",
        "print(\"-\" * 60)\n",
        "result3 = load_model_info(\"gpt-5-turbo\")\n",
        "\n",
        "if \"error\" in result3:\n",
        "    print(f\"‚ùå {result3['error']}\")\n",
        "    print(f\"   Available models: {', '.join(result3['available_models'][:3])}...\")\n",
        "else:\n",
        "    print(f\"‚úÖ Model loaded\")\n",
        "\n",
        "print()\n",
        "print(\"=\" * 60)\n",
        "print(\"‚úÖ Model Loader function is working correctly!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gpant5VtNj6D",
        "outputId": "467a7c07-6f71-4078-9612-a57ecc100e57"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ Testing Model Loader Function...\n",
            "\n",
            "Test 1: Load Gemini 1.5 Flash (approved)\n",
            "------------------------------------------------------------\n",
            "‚úÖ ‚úÖ Gemini 1.5 Flash is approved and ready to use\n",
            "   Provider: Google\n",
            "   Tier: standard\n",
            "   Input Cost: $0.000075 per 1K tokens\n",
            "   Output Cost: $0.000300 per 1K tokens\n",
            "\n",
            "Test 2: Try to load GPT-4 (deprecated)\n",
            "------------------------------------------------------------\n",
            "‚ùå Model 'gpt-4' is deprecated\n",
            "   Reason: Security review failed - data residency concerns\n",
            "   Use instead: gemini-1.5-pro\n",
            "\n",
            "Test 3: Try to load non-existent model\n",
            "------------------------------------------------------------\n",
            "‚ùå Model 'gpt-5-turbo' not found in registry\n",
            "   Available models: gemini-1.5-pro, gemini-1.5-flash, claude-3-opus...\n",
            "\n",
            "============================================================\n",
            "‚úÖ Model Loader function is working correctly!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# Cost Calculator Function\n",
        "# ========================================\n",
        "\n",
        "def calculate_cost(model_id, input_tokens, output_tokens):\n",
        "    \"\"\"\n",
        "    Calculate cost for using a specific model\n",
        "\n",
        "    Args:\n",
        "        model_id: ID of the model\n",
        "        input_tokens: Number of input tokens\n",
        "        output_tokens: Number of output tokens\n",
        "\n",
        "    Returns:\n",
        "        dict with cost breakdown\n",
        "    \"\"\"\n",
        "\n",
        "    # Load model info first (includes validation)\n",
        "    model_info = load_model_info(model_id)\n",
        "\n",
        "    # Check if model can be used\n",
        "    if \"error\" in model_info:\n",
        "        return {\n",
        "            \"error\": model_info[\"error\"],\n",
        "            \"suggestion\": model_info.get(\"replacement\", \"Choose an approved model\")\n",
        "        }\n",
        "\n",
        "    # Get costs\n",
        "    costs = model_info[\"costs\"]\n",
        "\n",
        "    # Calculate\n",
        "    input_cost = (input_tokens / 1000) * costs[\"input_cost_per_1k\"]\n",
        "    output_cost = (output_tokens / 1000) * costs[\"output_cost_per_1k\"]\n",
        "    total_cost = input_cost + output_cost\n",
        "\n",
        "    return {\n",
        "        \"model_id\": model_id,\n",
        "        \"model_name\": model_info[\"details\"][\"display_name\"],\n",
        "        \"breakdown\": {\n",
        "            \"input_tokens\": input_tokens,\n",
        "            \"input_cost\": input_cost,\n",
        "            \"output_tokens\": output_tokens,\n",
        "            \"output_cost\": output_cost,\n",
        "            \"total_cost\": total_cost\n",
        "        },\n",
        "        \"formatted\": f\"${total_cost:.4f}\",\n",
        "        \"cost_tier\": costs[\"cost_tier\"]\n",
        "    }\n",
        "\n",
        "\n",
        "def compare_model_costs(input_tokens, output_tokens, models=None):\n",
        "    \"\"\"\n",
        "    Compare costs across multiple models\n",
        "\n",
        "    Args:\n",
        "        input_tokens: Number of input tokens\n",
        "        output_tokens: Number of output tokens\n",
        "        models: List of model IDs to compare (default: all approved)\n",
        "\n",
        "    Returns:\n",
        "        list of cost comparisons, sorted by price\n",
        "    \"\"\"\n",
        "\n",
        "    # Default to all approved models\n",
        "    if models is None:\n",
        "        models = [\n",
        "            model_id for model_id, details in model_catalog.items()\n",
        "            if details[\"status\"] == \"approved\"\n",
        "        ]\n",
        "\n",
        "    # Calculate cost for each model\n",
        "    comparisons = []\n",
        "    for model_id in models:\n",
        "        result = calculate_cost(model_id, input_tokens, output_tokens)\n",
        "        if \"error\" not in result:\n",
        "            comparisons.append(result)\n",
        "\n",
        "    # Sort by cost (cheapest first)\n",
        "    comparisons.sort(key=lambda x: x[\"breakdown\"][\"total_cost\"])\n",
        "\n",
        "    return comparisons\n",
        "\n",
        "\n",
        "# ========================================\n",
        "# Test Cost Calculator\n",
        "# ========================================\n",
        "\n",
        "print(\"üß™ Testing Cost Calculator...\")\n",
        "print()\n",
        "\n",
        "# Test 1: Calculate cost for single model\n",
        "print(\"Test 1: Cost for 10,000 customer support messages\")\n",
        "print(\"Assumptions: 100 input tokens + 150 output tokens per message\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "messages = 10000\n",
        "input_per_message = 100\n",
        "output_per_message = 150\n",
        "\n",
        "total_input = messages * input_per_message   # 1M tokens\n",
        "total_output = messages * output_per_message # 1.5M tokens\n",
        "\n",
        "result = calculate_cost(\"gemini-1.5-flash\", total_input, total_output)\n",
        "\n",
        "if \"error\" not in result:\n",
        "    print(f\"Model: {result['model_name']}\")\n",
        "    print(f\"  Input: {result['breakdown']['input_tokens']:,} tokens ‚Üí ${result['breakdown']['input_cost']:.2f}\")\n",
        "    print(f\"  Output: {result['breakdown']['output_tokens']:,} tokens ‚Üí ${result['breakdown']['output_cost']:.2f}\")\n",
        "    print(f\"  Total Cost: ${result['breakdown']['total_cost']:.2f}\")\n",
        "    print(f\"  Cost per message: ${result['breakdown']['total_cost'] / messages:.4f}\")\n",
        "\n",
        "print()\n",
        "\n",
        "# Test 2: Compare all approved models\n",
        "print(\"Test 2: Compare costs across all approved models\")\n",
        "print(f\"Scenario: {total_input:,} input tokens + {total_output:,} output tokens\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "comparisons = compare_model_costs(total_input, total_output)\n",
        "\n",
        "print(f\"{'Model':<30} {'Total Cost':>12} {'Cost Tier':<15}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for i, comp in enumerate(comparisons, 1):\n",
        "    model_name = comp[\"model_name\"]\n",
        "    total_cost = comp[\"breakdown\"][\"total_cost\"]\n",
        "    tier = comp[\"cost_tier\"]\n",
        "\n",
        "    rank_emoji = \"ü•á\" if i == 1 else \"ü•à\" if i == 2 else \"ü•â\" if i == 3 else \"  \"\n",
        "    print(f\"{rank_emoji} {model_name:<28} ${total_cost:>10,.2f} {tier:<15}\")\n",
        "\n",
        "print()\n",
        "\n",
        "# Calculate savings\n",
        "if len(comparisons) > 1:\n",
        "    cheapest = comparisons[0][\"breakdown\"][\"total_cost\"]\n",
        "    most_expensive = comparisons[-1][\"breakdown\"][\"total_cost\"]\n",
        "    savings = most_expensive - cheapest\n",
        "    savings_percent = (savings / most_expensive) * 100\n",
        "\n",
        "    print(f\"üí° Insight:\")\n",
        "    print(f\"   Cheapest: {comparisons[0]['model_name']} (${cheapest:,.2f})\")\n",
        "    print(f\"   Most expensive: {comparisons[-1]['model_name']} (${most_expensive:,.2f})\")\n",
        "    print(f\"   Potential savings: ${savings:,.2f} ({savings_percent:.1f}% cheaper)\")\n",
        "\n",
        "print()\n",
        "\n",
        "# Test 3: Budget planning\n",
        "print(\"Test 3: Budget Planning - What can I afford?\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "monthly_budget = 1000  # $1,000/month\n",
        "messages_per_month = 50000\n",
        "\n",
        "input_per_msg = 100\n",
        "output_per_msg = 150\n",
        "\n",
        "total_input_monthly = messages_per_month * input_per_msg\n",
        "total_output_monthly = messages_per_month * output_per_msg\n",
        "\n",
        "print(f\"Budget: ${monthly_budget}/month\")\n",
        "print(f\"Expected volume: {messages_per_month:,} messages/month\")\n",
        "print(f\"Tokens per message: {input_per_msg} input + {output_per_msg} output\")\n",
        "print()\n",
        "\n",
        "comparisons = compare_model_costs(total_input_monthly, total_output_monthly)\n",
        "\n",
        "print(f\"{'Model':<30} {'Monthly Cost':>12} {'Within Budget?':<15}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for comp in comparisons:\n",
        "    model_name = comp[\"model_name\"]\n",
        "    monthly_cost = comp[\"breakdown\"][\"total_cost\"]\n",
        "\n",
        "    within_budget = monthly_cost <= monthly_budget\n",
        "    status = \"‚úÖ Yes\" if within_budget else \"‚ùå Over budget\"\n",
        "\n",
        "    print(f\"{model_name:<30} ${monthly_cost:>10,.2f} {status:<15}\")\n",
        "\n",
        "print()\n",
        "print(\"=\" * 70)\n",
        "print(\"‚úÖ Cost Calculator is working!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2cMbYDRQzHb",
        "outputId": "849af615-83e9-4e3b-bef6-99962bc295ad"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ Testing Cost Calculator...\n",
            "\n",
            "Test 1: Cost for 10,000 customer support messages\n",
            "Assumptions: 100 input tokens + 150 output tokens per message\n",
            "----------------------------------------------------------------------\n",
            "Model: Gemini 1.5 Flash\n",
            "  Input: 1,000,000 tokens ‚Üí $0.07\n",
            "  Output: 1,500,000 tokens ‚Üí $0.45\n",
            "  Total Cost: $0.52\n",
            "  Cost per message: $0.0001\n",
            "\n",
            "Test 2: Compare costs across all approved models\n",
            "Scenario: 1,000,000 input tokens + 1,500,000 output tokens\n",
            "----------------------------------------------------------------------\n",
            "Model                            Total Cost Cost Tier      \n",
            "----------------------------------------------------------------------\n",
            "ü•á Gemini 1.5 Flash             $      0.52 budget         \n",
            "ü•à Gemini 1.5 Pro               $      8.75 premium        \n",
            "ü•â Customer Support Model v1.2  $      8.75 premium        \n",
            "   Claude 3 Opus                $    127.50 premium-plus   \n",
            "\n",
            "üí° Insight:\n",
            "   Cheapest: Gemini 1.5 Flash ($0.52)\n",
            "   Most expensive: Claude 3 Opus ($127.50)\n",
            "   Potential savings: $126.97 (99.6% cheaper)\n",
            "\n",
            "Test 3: Budget Planning - What can I afford?\n",
            "----------------------------------------------------------------------\n",
            "Budget: $1000/month\n",
            "Expected volume: 50,000 messages/month\n",
            "Tokens per message: 100 input + 150 output\n",
            "\n",
            "Model                          Monthly Cost Within Budget? \n",
            "----------------------------------------------------------------------\n",
            "Gemini 1.5 Flash               $      2.62 ‚úÖ Yes          \n",
            "Gemini 1.5 Pro                 $     43.75 ‚úÖ Yes          \n",
            "Customer Support Model v1.2    $     43.75 ‚úÖ Yes          \n",
            "Claude 3 Opus                  $    637.50 ‚úÖ Yes          \n",
            "\n",
            "======================================================================\n",
            "‚úÖ Cost Calculator is working!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# COMPONENT 3: Quality Evaluation Framework (PRODUCTION-READY)\n",
        "# WITH REAL GEMINI API + READY FOR RAG/PEFT\n",
        "# ========================================\n",
        "\n",
        "print(\"üìä Building Quality Evaluation Framework...\")\n",
        "print()\n",
        "\n",
        "from datetime import datetime\n",
        "import json\n",
        "\n",
        "# ========================================\n",
        "# SETUP: Install and Configure Gemini API\n",
        "# ========================================\n",
        "\n",
        "print(\"Step 0: Setting up Gemini API...\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Install Google Generative AI SDK (free version)\n",
        "!pip install -q google-generativeai\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Configure API Key\n",
        "print(\"\\nüîë API Key Configuration\")\n",
        "print()\n",
        "\n",
        "GEMINI_API_KEY = input(\"Paste your LLM API key: \")\n",
        "\n",
        "if GEMINI_API_KEY.strip():\n",
        "    genai.configure(api_key=GEMINI_API_KEY)\n",
        "    USE_REAL_AI = True\n",
        "    print(\"‚úÖ Gemini API configured - will use REAL AI responses\")\n",
        "else:\n",
        "    USE_REAL_AI = False\n",
        "    print(\"‚ö†Ô∏è  No API key provided - will use simulated responses\")\n",
        "\n",
        "print()\n",
        "\n",
        "# ========================================\n",
        "# PART 1: Create Test Cases\n",
        "# ========================================\n",
        "\n",
        "print(\"Step 1: Creating Test Cases...\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Define test cases for customer support prompt\n",
        "customer_support_test_cases = [\n",
        "    {\n",
        "        \"test_id\": \"CS_001\",\n",
        "        \"category\": \"return_policy\",\n",
        "        \"input\": {\n",
        "            \"customer_question\": \"What is your return policy?\"\n",
        "        },\n",
        "        \"expected_elements\": [\n",
        "            \"30 days\",\n",
        "            \"receipt\",\n",
        "            \"original packaging\",\n",
        "            \"refund\"\n",
        "        ],\n",
        "        \"must_not_contain\": [\n",
        "            \"medical advice\",\n",
        "            \"legal counsel\",\n",
        "            \"guaranteed\"\n",
        "        ],\n",
        "        \"max_words\": 150,\n",
        "        \"quality_threshold\": 0.75\n",
        "    },\n",
        "\n",
        "    {\n",
        "        \"test_id\": \"CS_002\",\n",
        "        \"category\": \"policy_change\",\n",
        "        \"input\": {\n",
        "            \"customer_question\": \"Can I change my beneficiary?\"\n",
        "        },\n",
        "        \"expected_elements\": [\n",
        "            \"yes\",\n",
        "            \"beneficiary\",\n",
        "            \"form\",\n",
        "            \"contact\"\n",
        "        ],\n",
        "        \"must_not_contain\": [\n",
        "            \"cannot\",\n",
        "            \"impossible\",\n",
        "            \"never\"\n",
        "        ],\n",
        "        \"max_words\": 150,\n",
        "        \"quality_threshold\": 0.75\n",
        "    },\n",
        "\n",
        "    {\n",
        "        \"test_id\": \"CS_003\",\n",
        "        \"category\": \"premium_question\",\n",
        "        \"input\": {\n",
        "            \"customer_question\": \"Why did my premium increase?\"\n",
        "        },\n",
        "        \"expected_elements\": [\n",
        "            \"premium\",\n",
        "            \"age\",\n",
        "            \"policy\",\n",
        "            \"review\"\n",
        "        ],\n",
        "        \"must_not_contain\": [\n",
        "            \"your fault\",\n",
        "            \"penalized\",\n",
        "            \"unlucky\"\n",
        "        ],\n",
        "        \"max_words\": 150,\n",
        "        \"quality_threshold\": 0.5\n",
        "    },\n",
        "\n",
        "    {\n",
        "        \"test_id\": \"CS_004\",\n",
        "        \"category\": \"out_of_scope\",\n",
        "        \"input\": {\n",
        "            \"customer_question\": \"What should I invest in for retirement?\"\n",
        "        },\n",
        "        \"expected_elements\": [\n",
        "            \"financial advisor\",\n",
        "            \"consult\",\n",
        "            \"cannot provide investment advice\"\n",
        "        ],\n",
        "        \"must_not_contain\": [\n",
        "            \"buy stocks\",\n",
        "            \"invest in\",\n",
        "            \"recommend buying\"\n",
        "        ],\n",
        "        \"max_words\": 150,\n",
        "        \"quality_threshold\": 0.67,\n",
        "        \"should_escalate\": True\n",
        "    },\n",
        "\n",
        "    {\n",
        "        \"test_id\": \"CS_005\",\n",
        "        \"category\": \"empathy_test\",\n",
        "        \"input\": {\n",
        "            \"customer_question\": \"I'm upset because my claim was denied\"\n",
        "        },\n",
        "        \"expected_elements\": [\n",
        "            \"understand\",\n",
        "            \"sorry\",\n",
        "            \"help\",\n",
        "            \"review\"\n",
        "        ],\n",
        "        \"must_not_contain\": [\n",
        "            \"your fault\",\n",
        "            \"too bad\",\n",
        "            \"deal with it\"\n",
        "        ],\n",
        "        \"tone_check\": \"empathetic\",\n",
        "        \"max_words\": 150,\n",
        "        \"quality_threshold\": 0.75\n",
        "    }\n",
        "]\n",
        "\n",
        "# Save test cases\n",
        "test_cases_file = '/content/genaiops/evaluations/customer_support_test_cases.json'\n",
        "with open(test_cases_file, 'w') as f:\n",
        "    json.dump(customer_support_test_cases, f, indent=2)\n",
        "\n",
        "print(f\"‚úÖ Created {len(customer_support_test_cases)} test cases\")\n",
        "print(f\"   Saved to: {test_cases_file}\")\n",
        "print()\n",
        "\n",
        "# Display test cases summary\n",
        "print(\"Test Cases Summary:\")\n",
        "print(f\"{'Test ID':<12} {'Category':<20} {'Expected Elements':<10} {'Threshold':<10}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for test in customer_support_test_cases:\n",
        "    test_id = test[\"test_id\"]\n",
        "    category = test[\"category\"]\n",
        "    num_elements = len(test[\"expected_elements\"])\n",
        "    threshold = f\"{test['quality_threshold']:.0%}\"\n",
        "\n",
        "    print(f\"{test_id:<12} {category:<20} {num_elements:<10} {threshold:<10}\")\n",
        "\n",
        "print()\n",
        "\n",
        "# ========================================\n",
        "# PART 2: Evaluation Engine\n",
        "# ========================================\n",
        "\n",
        "print(\"Step 2: Building Evaluation Engine...\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "def evaluate_response(response_text, test_case):\n",
        "    \"\"\"\n",
        "    Evaluate an AI response against a test case\n",
        "\n",
        "    Args:\n",
        "        response_text: The AI's response (string)\n",
        "        test_case: Test case dict with expected_elements, must_not_contain, etc.\n",
        "\n",
        "    Returns:\n",
        "        Evaluation results dict\n",
        "    \"\"\"\n",
        "\n",
        "    response_lower = response_text.lower()\n",
        "\n",
        "    # Count expected elements present\n",
        "    expected_elements = test_case.get(\"expected_elements\", [])\n",
        "    elements_found = []\n",
        "    elements_missing = []\n",
        "\n",
        "    for element in expected_elements:\n",
        "        if element.lower() in response_lower:\n",
        "            elements_found.append(element)\n",
        "        else:\n",
        "            elements_missing.append(element)\n",
        "\n",
        "    # Check for forbidden elements\n",
        "    must_not_contain = test_case.get(\"must_not_contain\", [])\n",
        "    forbidden_found = []\n",
        "\n",
        "    for forbidden in must_not_contain:\n",
        "        if forbidden.lower() in response_lower:\n",
        "            forbidden_found.append(forbidden)\n",
        "\n",
        "    # Calculate score\n",
        "    if len(expected_elements) > 0:\n",
        "        score = len(elements_found) / len(expected_elements)\n",
        "    else:\n",
        "        score = 1.0\n",
        "\n",
        "    # Check if passes threshold\n",
        "    threshold = test_case.get(\"quality_threshold\", 0.8)\n",
        "    passed = score >= threshold and len(forbidden_found) == 0\n",
        "\n",
        "    # Check word count\n",
        "    word_count = len(response_text.split())\n",
        "    max_words = test_case.get(\"max_words\", 150)\n",
        "    word_count_ok = word_count <= max_words\n",
        "\n",
        "    # Overall pass/fail\n",
        "    overall_pass = passed and word_count_ok\n",
        "\n",
        "    return {\n",
        "        \"test_id\": test_case.get(\"test_id\", \"unknown\"),\n",
        "        \"category\": test_case.get(\"category\", \"unknown\"),\n",
        "        \"score\": score,\n",
        "        \"threshold\": threshold,\n",
        "        \"passed\": overall_pass,\n",
        "        \"details\": {\n",
        "            \"expected_elements\": {\n",
        "                \"total\": len(expected_elements),\n",
        "                \"found\": len(elements_found),\n",
        "                \"missing\": len(elements_missing),\n",
        "                \"found_list\": elements_found,\n",
        "                \"missing_list\": elements_missing\n",
        "            },\n",
        "            \"forbidden_elements\": {\n",
        "                \"total_forbidden\": len(must_not_contain),\n",
        "                \"found\": len(forbidden_found),\n",
        "                \"violations\": forbidden_found\n",
        "            },\n",
        "            \"word_count\": {\n",
        "                \"actual\": word_count,\n",
        "                \"max_allowed\": max_words,\n",
        "                \"ok\": word_count_ok\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ Evaluation engine created\")\n",
        "print()\n",
        "\n",
        "# ========================================\n",
        "# PART 3: AI Response Functions\n",
        "# ========================================\n",
        "\n",
        "def get_gemini_response(prompt, model_id=\"models/gemini-2.5-flash\"):\n",
        "    \"\"\"\n",
        "    Get response from Gemini API\n",
        "\n",
        "    Args:\n",
        "        prompt: The full prompt to send\n",
        "        model_id: Which Gemini model to use\n",
        "\n",
        "    Returns:\n",
        "        AI response text\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        model = genai.GenerativeModel(model_id)\n",
        "        response = model.generate_content(prompt)\n",
        "        return response.text\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"ERROR calling Gemini API: {str(e)}\"\n",
        "\n",
        "\n",
        "def get_rag_enhanced_response(prompt, context_data=None, model_id=\"models/gemini-2.5-flash\"):\n",
        "    \"\"\"\n",
        "    Get response from RAG-enhanced Gemini\n",
        "    (Placeholder for future RAG implementation)\n",
        "\n",
        "    Args:\n",
        "        prompt: User's question\n",
        "        context_data: Retrieved context from document database\n",
        "        model_id: Which Gemini model to use\n",
        "\n",
        "    Returns:\n",
        "        AI response with context\n",
        "    \"\"\"\n",
        "\n",
        "    if context_data:\n",
        "        enhanced_prompt = f\"\"\"Context from Prudential documents:\n",
        "{context_data}\n",
        "\n",
        "User Question:\n",
        "{prompt}\n",
        "\n",
        "Please answer based on the context provided above.\"\"\"\n",
        "\n",
        "        return get_gemini_response(enhanced_prompt, model_id)\n",
        "    else:\n",
        "        return get_gemini_response(prompt, model_id)\n",
        "\n",
        "\n",
        "def get_peft_tuned_response(prompt, tuned_model_id=None):\n",
        "    \"\"\"\n",
        "    Get response from PEFT fine-tuned Gemini\n",
        "    (Placeholder for future PEFT implementation)\n",
        "\n",
        "    Args:\n",
        "        prompt: The prompt\n",
        "        tuned_model_id: Your fine-tuned model ID from Vertex AI\n",
        "\n",
        "    Returns:\n",
        "        AI response from fine-tuned model\n",
        "    \"\"\"\n",
        "\n",
        "    if tuned_model_id:\n",
        "        return get_gemini_response(prompt, model_id=tuned_model_id)\n",
        "    else:\n",
        "        return get_gemini_response(prompt, model_id=\"models/gemini-2.5-flash\")\n",
        "\n",
        "\n",
        "def simulate_ai_response(test_case):\n",
        "    \"\"\"\n",
        "    Simulate AI responses (fallback when no API key)\n",
        "    \"\"\"\n",
        "\n",
        "    category = test_case.get(\"category\")\n",
        "\n",
        "    if category == \"return_policy\":\n",
        "        return \"\"\"Our return policy allows returns within 30 days of purchase with a valid receipt.\n",
        "        Items must be in original packaging and unused. You'll receive a full refund to your\n",
        "        original payment method. Next steps: 1) Visit our website, 2) Request a return label,\n",
        "        3) Ship the item back.\"\"\"\n",
        "\n",
        "    elif category == \"policy_change\":\n",
        "        return \"\"\"Yes, you can change your beneficiary at any time. You'll need to complete\n",
        "        a beneficiary change form. Next steps: 1) Contact your agent, 2) Complete the form,\n",
        "        3) Submit for processing.\"\"\"\n",
        "\n",
        "    elif category == \"premium_question\":\n",
        "        return \"\"\"Your premium may increase due to age, policy changes, or coverage adjustments.\n",
        "        I recommend reviewing your policy details with an agent.\"\"\"\n",
        "\n",
        "    elif category == \"out_of_scope\":\n",
        "        return \"\"\"I cannot provide specific investment advice as I'm not a licensed financial advisor.\n",
        "        For retirement planning, I recommend consulting with one of our certified financial advisors.\"\"\"\n",
        "\n",
        "    elif category == \"empathy_test\":\n",
        "        return \"\"\"I understand how frustrating this must be, and I'm truly sorry your claim was denied.\n",
        "        Let me help you understand why and explore your options.\"\"\"\n",
        "\n",
        "    else:\n",
        "        return \"This is a simulated response for testing purposes.\"\n",
        "\n",
        "\n",
        "# ========================================\n",
        "# PART 4: Automated Test Runner\n",
        "# ========================================\n",
        "\n",
        "print(\"Step 3: Building Automated Test Runner...\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "def run_test_suite(prompt_template, test_cases, model_config=None):\n",
        "    \"\"\"\n",
        "    Run a full test suite on a prompt\n",
        "\n",
        "    Args:\n",
        "        prompt_template: The prompt template to test\n",
        "        test_cases: List of test case dicts\n",
        "        model_config: Dict with model settings\n",
        "\n",
        "    Returns:\n",
        "        Test suite results\n",
        "    \"\"\"\n",
        "\n",
        "    # Default configuration\n",
        "    if model_config is None:\n",
        "        model_config = {\n",
        "            \"use_real_ai\": USE_REAL_AI,\n",
        "            \"model_type\": \"base\",\n",
        "            \"model_id\": \"models/gemini-2.5-flash\"\n",
        "        }\n",
        "\n",
        "    use_real_ai = model_config.get(\"use_real_ai\", USE_REAL_AI)\n",
        "    model_type = model_config.get(\"model_type\", \"base\")\n",
        "    model_id = model_config.get(\"model_id\", \"models/gemini-2.5-flash\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "    model_description = f\"{'REAL' if use_real_ai else 'SIMULATED'} {model_type.upper()} model\"\n",
        "    print(f\"Running {len(test_cases)} tests with {model_description}...\")\n",
        "    print()\n",
        "\n",
        "    for i, test_case in enumerate(test_cases, 1):\n",
        "        test_id = test_case.get(\"test_id\")\n",
        "        category = test_case.get(\"category\")\n",
        "\n",
        "        # Get input\n",
        "        customer_question = test_case[\"input\"][\"customer_question\"]\n",
        "\n",
        "        # Fill in prompt template\n",
        "        filled_prompt = prompt_template.format(customer_question=customer_question)\n",
        "\n",
        "        # Get response based on model type\n",
        "        if use_real_ai:\n",
        "            print(f\"  {i}. {test_id}: Calling {model_type.upper()} Gemini...\")\n",
        "\n",
        "            if model_type == \"rag\":\n",
        "                response = get_rag_enhanced_response(\n",
        "                    filled_prompt,\n",
        "                    context_data=model_config.get(\"context_data\"),\n",
        "                    model_id=model_id\n",
        "                )\n",
        "            elif model_type == \"peft\":\n",
        "                response = get_peft_tuned_response(\n",
        "                    filled_prompt,\n",
        "                    tuned_model_id=model_config.get(\"tuned_model_id\", model_id)\n",
        "                )\n",
        "            else:  # base\n",
        "                response = get_gemini_response(filled_prompt, model_id)\n",
        "        else:\n",
        "            response = simulate_ai_response(test_case)\n",
        "\n",
        "        # Evaluate response\n",
        "        evaluation = evaluate_response(response, test_case)\n",
        "        evaluation[\"input\"] = customer_question\n",
        "        evaluation[\"response\"] = response[:100] + \"...\" if len(response) > 100 else response\n",
        "        evaluation[\"full_response\"] = response\n",
        "\n",
        "        results.append(evaluation)\n",
        "\n",
        "        # Print progress\n",
        "        status = \"‚úÖ PASS\" if evaluation[\"passed\"] else \"‚ùå FAIL\"\n",
        "        print(f\"     {status} - Score: {evaluation['score']:.0%}\")\n",
        "\n",
        "    print()\n",
        "\n",
        "    # Calculate overall stats\n",
        "    total_tests = len(results)\n",
        "    passed_tests = len([r for r in results if r[\"passed\"]])\n",
        "    failed_tests = total_tests - passed_tests\n",
        "    pass_rate = passed_tests / total_tests if total_tests > 0 else 0\n",
        "\n",
        "    return {\n",
        "        \"test_suite_name\": \"Customer Support Prompt Evaluation\",\n",
        "        \"model_type\": model_type,\n",
        "        \"model_id\": model_id,\n",
        "        \"ai_mode\": \"real\" if use_real_ai else \"simulated\",\n",
        "        \"run_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "        \"total_tests\": total_tests,\n",
        "        \"passed\": passed_tests,\n",
        "        \"failed\": failed_tests,\n",
        "        \"pass_rate\": pass_rate,\n",
        "        \"results\": results\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ Test runner created\")\n",
        "print()\n",
        "\n",
        "# ========================================\n",
        "# PART 5: Quality Report Generator\n",
        "# ========================================\n",
        "\n",
        "print(\"Step 4: Building Quality Report Generator...\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "def generate_quality_report(test_results):\n",
        "    \"\"\"\n",
        "    Generate a quality report from test results\n",
        "    \"\"\"\n",
        "\n",
        "    print()\n",
        "    print(\"=\" * 70)\n",
        "    print(\"QUALITY EVALUATION REPORT\")\n",
        "    print(\"=\" * 70)\n",
        "    print()\n",
        "\n",
        "    print(f\"Test Suite: {test_results['test_suite_name']}\")\n",
        "    print(f\"Model: {test_results.get('model_id', 'N/A')} ({test_results.get('model_type', 'base')})\")\n",
        "    print(f\"AI Mode: {test_results.get('ai_mode', 'N/A').upper()}\")\n",
        "    print(f\"Run Date: {test_results['run_date']}\")\n",
        "    print()\n",
        "\n",
        "    print(\"OVERALL RESULTS\")\n",
        "    print(\"-\" * 70)\n",
        "    print(f\"Total Tests: {test_results['total_tests']}\")\n",
        "    print(f\"Passed: {test_results['passed']} ‚úÖ\")\n",
        "    print(f\"Failed: {test_results['failed']} ‚ùå\")\n",
        "    print(f\"Pass Rate: {test_results['pass_rate']:.1%}\")\n",
        "    print()\n",
        "\n",
        "    # Category breakdown\n",
        "    print(\"RESULTS BY CATEGORY\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    categories = {}\n",
        "    for result in test_results['results']:\n",
        "        cat = result['category']\n",
        "        if cat not in categories:\n",
        "            categories[cat] = {\"passed\": 0, \"failed\": 0}\n",
        "\n",
        "        if result['passed']:\n",
        "            categories[cat]['passed'] += 1\n",
        "        else:\n",
        "            categories[cat]['failed'] += 1\n",
        "\n",
        "    for category, stats in categories.items():\n",
        "        total = stats['passed'] + stats['failed']\n",
        "        pass_rate = stats['passed'] / total if total > 0 else 0\n",
        "        print(f\"{category:<20} {stats['passed']}/{total} passed ({pass_rate:.0%})\")\n",
        "\n",
        "    print()\n",
        "\n",
        "    # Failed tests detail\n",
        "    failed_results = [r for r in test_results['results'] if not r['passed']]\n",
        "\n",
        "    if failed_results:\n",
        "        print(\"FAILED TESTS DETAIL\")\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "        for result in failed_results:\n",
        "            print(f\"\\nTest: {result['test_id']} ({result['category']})\")\n",
        "            print(f\"Score: {result['score']:.0%} (threshold: {result['threshold']:.0%})\")\n",
        "            print(f\"Missing elements: {result['details']['expected_elements']['missing_list']}\")\n",
        "            if result['details']['forbidden_elements']['violations']:\n",
        "                print(f\"‚ö†Ô∏è  Violations: {result['details']['forbidden_elements']['violations']}\")\n",
        "    else:\n",
        "        print(\"‚úÖ All tests passed!\")\n",
        "\n",
        "    print()\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Save report\n",
        "    report_file = f'/content/genaiops/evaluations/quality_report_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json'\n",
        "    with open(report_file, 'w') as f:\n",
        "        json.dump(test_results, f, indent=2)\n",
        "\n",
        "    print(f\"üìÑ Full report saved to: {report_file}\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "print(\"‚úÖ Report generator created\")\n",
        "print()\n",
        "\n",
        "# ========================================\n",
        "# TEST THE COMPLETE SYSTEM\n",
        "# ========================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üß™ TESTING COMPLETE QUALITY EVALUATION SYSTEM\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Customer support prompt\n",
        "test_prompt = \"\"\"You are a helpful customer service representative for Prudential Financial.\n",
        "\n",
        "Customer Question:\n",
        "{customer_question}\n",
        "\n",
        "Instructions:\n",
        "- Be professional and empathetic\n",
        "- Provide accurate information\n",
        "- Keep response under 150 words\n",
        "- Include next steps\n",
        "\n",
        "Response:\"\"\"\n",
        "\n",
        "# Run the test suite\n",
        "test_results = run_test_suite(\n",
        "    prompt_template=test_prompt,\n",
        "    test_cases=customer_support_test_cases,\n",
        "    model_config={\n",
        "        \"use_real_ai\": USE_REAL_AI,\n",
        "        \"model_type\": \"base\",\n",
        "        \"model_id\": \"models/gemini-2.5-flash\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# Generate quality report\n",
        "generate_quality_report(test_results)\n",
        "\n",
        "print()\n",
        "print(\"=\" * 70)\n",
        "print(\"‚úÖ COMPONENT 3: QUALITY EVALUATION - CORE COMPLETE!\")\n",
        "print(\"=\" * 70)\n",
        "print()\n",
        "print(\"What we built:\")\n",
        "print(\"  ‚úÖ Test cases (5 test scenarios)\")\n",
        "print(\"  ‚úÖ Evaluation engine (works with any model)\")\n",
        "print(\"  ‚úÖ Real Gemini API integration (Gemini 2.5 Flash)\")\n",
        "print(\"  ‚úÖ Support for future RAG enhancement\")\n",
        "print(\"  ‚úÖ Support for future PEFT fine-tuning\")\n",
        "print(\"  ‚úÖ Automated test runner\")\n",
        "print(\"  ‚úÖ Quality report generator\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Xy2cL0uD6W1z",
        "outputId": "c9b427cd-ca4c-47d9-cbb3-5a4ae49ca5c8"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Building Quality Evaluation Framework...\n",
            "\n",
            "Step 0: Setting up Gemini API...\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "üîë API Key Configuration\n",
            "\n",
            "Paste your LLM API key: AIzaSyDaTyV8PcW-IZRupTKMacQya85apagPzo4\n",
            "‚úÖ Gemini API configured - will use REAL AI responses\n",
            "\n",
            "Step 1: Creating Test Cases...\n",
            "----------------------------------------------------------------------\n",
            "‚úÖ Created 5 test cases\n",
            "   Saved to: /content/genaiops/evaluations/customer_support_test_cases.json\n",
            "\n",
            "Test Cases Summary:\n",
            "Test ID      Category             Expected Elements Threshold \n",
            "----------------------------------------------------------------------\n",
            "CS_001       return_policy        4          75%       \n",
            "CS_002       policy_change        4          75%       \n",
            "CS_003       premium_question     4          50%       \n",
            "CS_004       out_of_scope         3          67%       \n",
            "CS_005       empathy_test         4          75%       \n",
            "\n",
            "Step 2: Building Evaluation Engine...\n",
            "----------------------------------------------------------------------\n",
            "‚úÖ Evaluation engine created\n",
            "\n",
            "Step 3: Building Automated Test Runner...\n",
            "----------------------------------------------------------------------\n",
            "‚úÖ Test runner created\n",
            "\n",
            "Step 4: Building Quality Report Generator...\n",
            "----------------------------------------------------------------------\n",
            "‚úÖ Report generator created\n",
            "\n",
            "\n",
            "======================================================================\n",
            "üß™ TESTING COMPLETE QUALITY EVALUATION SYSTEM\n",
            "======================================================================\n",
            "Running 5 tests with REAL BASE model...\n",
            "\n",
            "  1. CS_001: Calling BASE Gemini...\n",
            "     ‚ùå FAIL - Score: 50%\n",
            "  2. CS_002: Calling BASE Gemini...\n",
            "     ‚úÖ PASS - Score: 75%\n",
            "  3. CS_003: Calling BASE Gemini...\n",
            "     ‚úÖ PASS - Score: 100%\n",
            "  4. CS_004: Calling BASE Gemini...\n",
            "     ‚ùå FAIL - Score: 33%\n",
            "  5. CS_005: Calling BASE Gemini...\n",
            "     ‚úÖ PASS - Score: 100%\n",
            "\n",
            "\n",
            "======================================================================\n",
            "QUALITY EVALUATION REPORT\n",
            "======================================================================\n",
            "\n",
            "Test Suite: Customer Support Prompt Evaluation\n",
            "Model: models/gemini-2.5-flash (base)\n",
            "AI Mode: REAL\n",
            "Run Date: 2026-02-15 10:49:34\n",
            "\n",
            "OVERALL RESULTS\n",
            "----------------------------------------------------------------------\n",
            "Total Tests: 5\n",
            "Passed: 3 ‚úÖ\n",
            "Failed: 2 ‚ùå\n",
            "Pass Rate: 60.0%\n",
            "\n",
            "RESULTS BY CATEGORY\n",
            "----------------------------------------------------------------------\n",
            "return_policy        0/1 passed (0%)\n",
            "policy_change        1/1 passed (100%)\n",
            "premium_question     1/1 passed (100%)\n",
            "out_of_scope         0/1 passed (0%)\n",
            "empathy_test         1/1 passed (100%)\n",
            "\n",
            "FAILED TESTS DETAIL\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Test: CS_001 (return_policy)\n",
            "Score: 50% (threshold: 75%)\n",
            "Missing elements: ['receipt', 'original packaging']\n",
            "\n",
            "Test: CS_004 (out_of_scope)\n",
            "Score: 33% (threshold: 67%)\n",
            "Missing elements: ['financial advisor', 'cannot provide investment advice']\n",
            "\n",
            "======================================================================\n",
            "üìÑ Full report saved to: /content/genaiops/evaluations/quality_report_20260215_104934.json\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "‚úÖ COMPONENT 3: QUALITY EVALUATION - CORE COMPLETE!\n",
            "======================================================================\n",
            "\n",
            "What we built:\n",
            "  ‚úÖ Test cases (5 test scenarios)\n",
            "  ‚úÖ Evaluation engine (works with any model)\n",
            "  ‚úÖ Real Gemini API integration (Gemini 2.5 Flash)\n",
            "  ‚úÖ Support for future RAG enhancement\n",
            "  ‚úÖ Support for future PEFT fine-tuning\n",
            "  ‚úÖ Automated test runner\n",
            "  ‚úÖ Quality report generator\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R6DuF7jx51OR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}