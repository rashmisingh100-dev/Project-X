{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbBTP53/BchrK67YZzkhFI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rashmisingh100-dev/Project-X/blob/main/GenAIOps_Framework_Module1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFavuHwW2qXp",
        "outputId": "792ce50d-2ebe-4459-c94d-3f87cb59231c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GenAIOps Framework - Starting Setup\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "#GenAI Ops Framework - Module 1: Foundation\n",
        "#This module builds the core GenAI components\n",
        "print(\"GenAIOps Framework - Starting Setup\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create Visual Directory Setup\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "Aaugcm203HlT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Setup directories\n",
        "base_dir= Path('/content/genaiops')\n",
        "base_dir.mkdir(exist_ok=True)"
      ],
      "metadata": {
        "id": "XyDJvnhe3Csp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create subsidirectories\n",
        "(base_dir/'prompts').mkdir(exist_ok=True)\n",
        "(base_dir / 'models').mkdir(exist_ok=True)\n",
        "(base_dir / 'evaluations').mkdir(exist_ok=True)\n",
        "(base_dir / 'logs').mkdir(exist_ok=True)"
      ],
      "metadata": {
        "id": "_-ISBxDc36Nj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"‚úÖ Environment ready!\")\n",
        "print(f\"üìÅ Base directory: {base_dir}\")\n",
        "print()\n",
        "print(\"üëâ You can now run the rest of the notebook\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_7XCAwW3jLH",
        "outputId": "d8672be3-6778-4f81-b35a-1d04691be143"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Environment ready!\n",
            "üìÅ Base directory: /content/genaiops\n",
            "\n",
            "üëâ You can now run the rest of the notebook\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#verify setup worked\n",
        "import os\n",
        "print(\"üîç Verifying GenAIOps directory structure...\")\n",
        "print()\n",
        "\n",
        "for folder in ['prompts', 'models', 'evaluations', 'logs']:\n",
        "    path = f'/content/genaiops/{folder}'\n",
        "    exists = os.path.exists(path)\n",
        "    status = \"‚úÖ\" if exists else \"‚ùå\"\n",
        "    print(f\"{status} {path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3ZmVuNXAXc-",
        "outputId": "897b047b-a701-48ce-b379-8162c99e06f1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Verifying GenAIOps directory structure...\n",
            "\n",
            "‚úÖ /content/genaiops/prompts\n",
            "‚úÖ /content/genaiops/models\n",
            "‚úÖ /content/genaiops/evaluations\n",
            "‚úÖ /content/genaiops/logs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# COMPONENT 1: Prompt Management System\n",
        "# ========================================\n",
        "print(\"üìù Building Prompt Management System...\")\n",
        "print()\n",
        "#Define Prompt Template for Customer Support\n",
        "customer_support_prompt_v1 = \"\"\"\n",
        "You are a helpful customer service representative for Prudential Financial.\n",
        "\n",
        "Customer Question:{customer_question}\n",
        "\n",
        "Instructions:\n",
        "- Be professional and empathetic\n",
        "- Provide accurate information about policies, only factual and grounded answer with no hallucination\n",
        "- If you don't know the answer, say so clearly\n",
        "- Keep response under 150 words\n",
        "- Include next steps when applicable\n",
        "- Professional yet conversational tone\n",
        "- Include 2-3 specific next steps\n",
        "- Offer specialist escalation if complex\n",
        "\n",
        "Safety Rules:\n",
        "- Never provide medical advice\n",
        "- Never make financial predictions\n",
        "- Don't discuss other customers\n",
        "- Escalate legal questions to compliance team\n",
        "Response:\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpfeMYoEAvBE",
        "outputId": "4731bd2e-42e5-43a9-8a54-e311b341653f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìù Building Prompt Management System...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save this prompt to our prompts directory\n",
        "prompt_file_path = '/content/genaiops/prompts/customer_support_v1.0.txt'\n",
        "\n",
        "with open(prompt_file_path, 'w') as f:\n",
        "    f.write(customer_support_prompt_v1)\n",
        "\n",
        "print(f\"‚úÖ Prompt saved to: {prompt_file_path}\")\n",
        "print()\n",
        "print(\"üìÑ Prompt content:\")\n",
        "print(\"-\" * 50)\n",
        "print(customer_support_prompt_v1)\n"
      ],
      "metadata": {
        "id": "9cf-fKg8FgHE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcba7b02-7b6b-4330-abd2-2d3046a2641d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Prompt saved to: /content/genaiops/prompts/customer_support_v1.0.txt\n",
            "\n",
            "üìÑ Prompt content:\n",
            "--------------------------------------------------\n",
            "\n",
            "You are a helpful customer service representative for Prudential Financial.\n",
            "\n",
            "Customer Question:{customer_question}\n",
            "\n",
            "Instructions:\n",
            "- Be professional and empathetic\n",
            "- Provide accurate information about policies, only factual and grounded answer with no hallucination\n",
            "- If you don't know the answer, say so clearly\n",
            "- Keep response under 150 words\n",
            "- Include next steps when applicable\n",
            "- Professional yet conversational tone\n",
            "- Include 2-3 specific next steps\n",
            "- Offer specialist escalation if complex\n",
            "\n",
            "Safety Rules:\n",
            "- Never provide medical advice\n",
            "- Never make financial predictions\n",
            "- Don't discuss other customers\n",
            "- Escalate legal questions to compliance team\n",
            "Response:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prompt Metadata (Governance)\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# Create metadata for our prompt\n",
        "prompt_metadata = {\n",
        "    \"prompt_id\": \"customer_support_v1.0\",\n",
        "    \"version\": \"1.0\",\n",
        "    \"created_date\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
        "    \"created_by\": \"Rashmi Singh\",\n",
        "    \"status\": \"approved\",\n",
        "    \"use_case\": \"Customer service chatbot\",\n",
        "    \"model_compatibility\": [\"gemini-1.5-pro\", \"gemini-1.5-flash\"],\n",
        "    \"approved_by\": \"Data & AI COE (Group)\",\n",
        "    \"approval_date\": \"2024-02-14\",\n",
        "    \"description\": \"Professional customer service prompt with empathy and accuracy focus\",\n",
        "    \"test_pass_rate\": 0.95,  # 95% of test cases passed\n",
        "    \"production_apps\": [\"CustomerSupportBot\", \"EmailAutomation\"]\n",
        "}\n",
        "\n",
        "# Save metadata as JSON\n",
        "metadata_file = '/content/genaiops/prompts/customer_support_v1.0_metadata.json'\n",
        "\n",
        "with open(metadata_file, 'w') as f:\n",
        "    json.dump(prompt_metadata, f, indent=2)\n",
        "\n",
        "print(\"‚úÖ Prompt metadata saved\")\n",
        "print()\n",
        "print(\"üìã Metadata:\")\n",
        "print(json.dumps(prompt_metadata, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FLL4wkWmlYK",
        "outputId": "82785636-0814-4dbf-a39f-5b087d3d3f81"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Prompt metadata saved\n",
            "\n",
            "üìã Metadata:\n",
            "{\n",
            "  \"prompt_id\": \"customer_support_v1.0\",\n",
            "  \"version\": \"1.0\",\n",
            "  \"created_date\": \"2026-02-15\",\n",
            "  \"created_by\": \"Rashmi Singh\",\n",
            "  \"status\": \"approved\",\n",
            "  \"use_case\": \"Customer service chatbot\",\n",
            "  \"model_compatibility\": [\n",
            "    \"gemini-1.5-pro\",\n",
            "    \"gemini-1.5-flash\"\n",
            "  ],\n",
            "  \"approved_by\": \"Data & AI COE (Group)\",\n",
            "  \"approval_date\": \"2024-02-14\",\n",
            "  \"description\": \"Professional customer service prompt with empathy and accuracy focus\",\n",
            "  \"test_pass_rate\": 0.95,\n",
            "  \"production_apps\": [\n",
            "    \"CustomerSupportBot\",\n",
            "    \"EmailAutomation\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prompt Loader Function\n",
        "\n",
        "def load_prompt(prompt_id, version=\"latest\"):\n",
        "    \"\"\"\n",
        "    Load a prompt template by ID and version\n",
        "\n",
        "    Args:\n",
        "        prompt_id: Name of the prompt (e.g., 'customer_support')\n",
        "        version: Version number (e.g., '1.0') or 'latest'\n",
        "\n",
        "    Returns:\n",
        "        dict with 'template' and 'metadata'\n",
        "    \"\"\"\n",
        "\n",
        "    # Construct file paths\n",
        "    if version == \"latest\":\n",
        "        # In real system, would query database for latest version\n",
        "        # For now, we'll use v1.0\n",
        "        version = \"1.0\"\n",
        "\n",
        "    prompt_file = f'/content/genaiops/prompts/{prompt_id}_v{version}.txt'\n",
        "    metadata_file = f'/content/genaiops/prompts/{prompt_id}_v{version}_metadata.json'\n",
        "\n",
        "    # Load prompt template\n",
        "    try:\n",
        "        with open(prompt_file, 'r') as f:\n",
        "            template = f.read()\n",
        "    except FileNotFoundError:\n",
        "        return {\"error\": f\"Prompt {prompt_id} v{version} not found\"}\n",
        "\n",
        "    # Load metadata\n",
        "    try:\n",
        "        with open(metadata_file, 'r') as f:\n",
        "            metadata = json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        metadata = {\"warning\": \"No metadata found\"}\n",
        "\n",
        "    return {\n",
        "        \"template\": template,\n",
        "        \"metadata\": metadata\n",
        "    }\n",
        "\n",
        "\n",
        "# Test the loader\n",
        "print(\"üß™ Testing prompt loader...\")\n",
        "print()\n",
        "\n",
        "result = load_prompt(\"customer_support\", version=\"1.0\")\n",
        "\n",
        "print(\"‚úÖ Prompt loaded successfully!\")\n",
        "print()\n",
        "print(\"üìÑ Template:\")\n",
        "print(result['template'][:200] + \"...\")  # First 200 chars\n",
        "print()\n",
        "print(\"üìã Metadata:\")\n",
        "print(f\"  Version: {result['metadata']['version']}\")\n",
        "print(f\"  Status: {result['metadata']['status']}\")\n",
        "print(f\"  Use Case: {result['metadata']['use_case']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4w5KD_4xq2Wf",
        "outputId": "6353ac7a-795d-43e1-9363-dbd3f8b0fb1b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ Testing prompt loader...\n",
            "\n",
            "‚úÖ Prompt loaded successfully!\n",
            "\n",
            "üìÑ Template:\n",
            "\n",
            "You are a helpful customer service representative for Prudential Financial.\n",
            "\n",
            "Customer Question:{customer_question}\n",
            "\n",
            "Instructions:\n",
            "- Be professional and empathetic\n",
            "- Provide accurate information about...\n",
            "\n",
            "üìã Metadata:\n",
            "  Version: 1.0\n",
            "  Status: approved\n",
            "  Use Case: Customer service chatbot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prompt Version Comparison Tool\n",
        "# Create an improved version (v1.1)\n",
        "customer_support_prompt_v1_1 = \"\"\"\n",
        "You are an empathetic customer service representative for Prudential Financial with deep knowledge of our insurance products and policies.\n",
        "\n",
        "Customer Profile:\n",
        "- Name: {customer_name}\n",
        "- Policy Type: {policy_type}\n",
        "- Customer Since: {customer_since}\n",
        "\n",
        "Customer Question:\n",
        "{customer_question}\n",
        "\n",
        "Instructions:\n",
        "- Address customer by name to personalize the response\n",
        "- Be professional, empathetic, and solution-oriented\n",
        "- Reference their specific policy type when relevant\n",
        "- Provide accurate information about Prudential policies\n",
        "- If you don't know the answer, be honest and offer to connect them with a specialist\n",
        "- Keep response under 150 words\n",
        "- Always include clear next steps\n",
        "- End with \"Is there anything else I can help you with today?\"\n",
        "- Professional yet conversational tone\n",
        "- Include 2-3 specific next steps\n",
        "- Offer specialist escalation if complex\n",
        "\n",
        "Safety Rules:\n",
        "- Never provide medical advice\n",
        "- Never make financial predictions\n",
        "- Don't discuss other customers\n",
        "- Escalate legal questions to compliance team\n",
        "\n",
        "Response:\n",
        "\"\"\"\n",
        "\n",
        "# Save v1.1\n",
        "prompt_v1_1_path = '/content/genaiops/prompts/customer_support_v1.1.txt'\n",
        "with open(prompt_v1_1_path, 'w') as f:\n",
        "    f.write(customer_support_prompt_v1_1)\n",
        "\n",
        "# Create metadata for v1.1\n",
        "metadata_v1_1 = {\n",
        "    \"prompt_id\": \"customer_support_v1.1\",\n",
        "    \"version\": \"1.1\",\n",
        "    \"created_date\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
        "    \"created_by\": \"Rashmi Singh\",\n",
        "    \"status\": \"testing\",  # Not yet approved for production\n",
        "    \"use_case\": \"Customer service chatbot\",\n",
        "    \"model_compatibility\": [\"gemini-1.5-pro\", \"gemini-1.5-flash\"],\n",
        "    \"description\": \"Enhanced with personalization and policy-type awareness\",\n",
        "    \"improvements_over_v1.0\": [\n",
        "        \"Personalization with customer name\",\n",
        "        \"Policy-type specific responses\",\n",
        "        \"Customer tenure awareness\",\n",
        "        \"Standardized closing question\"\n",
        "    ],\n",
        "    \"test_pass_rate\": None,  # Not yet tested\n",
        "    \"production_apps\": []  # Not yet deployed\n",
        "}\n",
        "\n",
        "metadata_v1_1_path = '/content/genaiops/prompts/customer_support_v1.1_metadata.json'\n",
        "with open(metadata_v1_1_path, 'w') as f:\n",
        "    json.dump(metadata_v1_1, f, indent=2)\n",
        "\n",
        "print(\"‚úÖ Created prompt v1.1 (improved version)\")\n",
        "print()\n",
        "print(\"üÜö Comparing v1.0 vs v1.1:\")\n",
        "print(\"-\" * 60)\n",
        "print(\"v1.0 (Production):\")\n",
        "print(\"  - Generic customer addressing\")\n",
        "print(\"  - No personalization\")\n",
        "print(\"  - Status: Approved ‚úÖ\")\n",
        "print()\n",
        "print(\"v1.1 (Testing):\")\n",
        "print(\"  - Personalized with customer name\")\n",
        "print(\"  - Policy-type aware\")\n",
        "print(\"  - Customer tenure aware\")\n",
        "print(\"  - Standardized closing\")\n",
        "print(\"  - Status: Testing üß™\")\n",
        "print()\n",
        "print(\"üìä Next step: A/B testing to compare quality\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NWdsm_grCZO",
        "outputId": "d64528e6-062d-406c-977c-91c16c0d2af3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Created prompt v1.1 (improved version)\n",
            "\n",
            "üÜö Comparing v1.0 vs v1.1:\n",
            "------------------------------------------------------------\n",
            "v1.0 (Production):\n",
            "  - Generic customer addressing\n",
            "  - No personalization\n",
            "  - Status: Approved ‚úÖ\n",
            "\n",
            "v1.1 (Testing):\n",
            "  - Personalized with customer name\n",
            "  - Policy-type aware\n",
            "  - Customer tenure aware\n",
            "  - Standardized closing\n",
            "  - Status: Testing üß™\n",
            "\n",
            "üìä Next step: A/B testing to compare quality\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# FEATURE 1: A/B Testing Framework\n",
        "# ========================================\n",
        "\n",
        "print(\"üî¨ Building A/B Testing Framework for Prompts...\")\n",
        "print()\n",
        "\n",
        "import hashlib\n",
        "import random\n",
        "\n",
        "class ABTestManager:\n",
        "    \"\"\"\n",
        "    Manages A/B tests for prompt versions\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.active_tests = {}\n",
        "        self.test_results = {}\n",
        "\n",
        "    def create_ab_test(self, test_id, prompt_id, variant_a_version,\n",
        "                       variant_b_version, traffic_split=0.5):\n",
        "        \"\"\"\n",
        "        Create a new A/B test\n",
        "\n",
        "        Args:\n",
        "            test_id: Unique test identifier\n",
        "            prompt_id: Which prompt to test\n",
        "            variant_a_version: Control version (e.g., \"1.0\")\n",
        "            variant_b_version: Treatment version (e.g., \"1.1\")\n",
        "            traffic_split: % of traffic to variant B (0.0 to 1.0)\n",
        "        \"\"\"\n",
        "\n",
        "        self.active_tests[test_id] = {\n",
        "            \"test_id\": test_id,\n",
        "            \"prompt_id\": prompt_id,\n",
        "            \"variant_a\": {\n",
        "                \"version\": variant_a_version,\n",
        "                \"traffic\": 1 - traffic_split,\n",
        "                \"requests\": 0,\n",
        "                \"label\": \"Control (A)\"\n",
        "            },\n",
        "            \"variant_b\": {\n",
        "                \"version\": variant_b_version,\n",
        "                \"traffic\": traffic_split,\n",
        "                \"requests\": 0,\n",
        "                \"label\": \"Treatment (B)\"\n",
        "            },\n",
        "            \"status\": \"active\",\n",
        "            \"created_date\": \"2024-02-14\",\n",
        "            \"total_requests\": 0\n",
        "        }\n",
        "\n",
        "        # Save test configuration\n",
        "        test_file = f'/content/genaiops/prompts/ab_test_{test_id}.json'\n",
        "        with open(test_file, 'w') as f:\n",
        "            json.dump(self.active_tests[test_id], f, indent=2)\n",
        "\n",
        "        print(f\"‚úÖ A/B Test Created: {test_id}\")\n",
        "        print(f\"   Prompt: {prompt_id}\")\n",
        "        print(f\"   Variant A (Control): v{variant_a_version} - {(1-traffic_split)*100:.0f}% traffic\")\n",
        "        print(f\"   Variant B (Treatment): v{variant_b_version} - {traffic_split*100:.0f}% traffic\")\n",
        "\n",
        "        return self.active_tests[test_id]\n",
        "\n",
        "    def assign_variant(self, test_id, user_id):\n",
        "        \"\"\"\n",
        "        Assign a user to variant A or B\n",
        "        Uses consistent hashing so same user always gets same variant\n",
        "\n",
        "        Args:\n",
        "            test_id: Which test\n",
        "            user_id: User identifier (email, customer ID, etc.)\n",
        "\n",
        "        Returns:\n",
        "            dict with assigned variant info\n",
        "        \"\"\"\n",
        "\n",
        "        if test_id not in self.active_tests:\n",
        "            return {\"error\": f\"Test {test_id} not found\"}\n",
        "\n",
        "        test = self.active_tests[test_id]\n",
        "\n",
        "        # Consistent hashing: same user_id always gets same variant\n",
        "        hash_input = f\"{test_id}:{user_id}\".encode()\n",
        "        hash_value = int(hashlib.md5(hash_input).hexdigest(), 16)\n",
        "        user_hash = (hash_value % 100) / 100  # 0.00 to 0.99\n",
        "\n",
        "        # Assign variant based on traffic split\n",
        "        if user_hash < test[\"variant_b\"][\"traffic\"]:\n",
        "            assigned_variant = \"B\"\n",
        "            version = test[\"variant_b\"][\"version\"]\n",
        "            test[\"variant_b\"][\"requests\"] += 1\n",
        "        else:\n",
        "            assigned_variant = \"A\"\n",
        "            version = test[\"variant_a\"][\"version\"]\n",
        "            test[\"variant_a\"][\"requests\"] += 1\n",
        "\n",
        "        test[\"total_requests\"] += 1\n",
        "\n",
        "        return {\n",
        "            \"test_id\": test_id,\n",
        "            \"user_id\": user_id,\n",
        "            \"assigned_variant\": assigned_variant,\n",
        "            \"prompt_version\": version,\n",
        "            \"prompt_id\": test[\"prompt_id\"]\n",
        "        }\n",
        "\n",
        "    def get_prompt_for_user(self, test_id, user_id):\n",
        "        \"\"\"\n",
        "        Get the appropriate prompt version for a user in an A/B test\n",
        "\n",
        "        Args:\n",
        "            test_id: Which test\n",
        "            user_id: User identifier\n",
        "\n",
        "        Returns:\n",
        "            Prompt template for the assigned variant\n",
        "        \"\"\"\n",
        "\n",
        "        # Assign variant\n",
        "        assignment = self.assign_variant(test_id, user_id)\n",
        "\n",
        "        if \"error\" in assignment:\n",
        "            return assignment\n",
        "\n",
        "        # Load the appropriate prompt version\n",
        "        prompt_id = assignment[\"prompt_id\"]\n",
        "        version = assignment[\"prompt_version\"]\n",
        "\n",
        "        prompt_data = load_prompt(prompt_id, version)\n",
        "\n",
        "        return {\n",
        "            \"assignment\": assignment,\n",
        "            \"prompt\": prompt_data\n",
        "        }\n",
        "\n",
        "    def get_test_stats(self, test_id):\n",
        "        \"\"\"\n",
        "        Get statistics for an A/B test\n",
        "        \"\"\"\n",
        "\n",
        "        if test_id not in self.active_tests:\n",
        "            return {\"error\": f\"Test {test_id} not found\"}\n",
        "\n",
        "        test = self.active_tests[test_id]\n",
        "\n",
        "        return {\n",
        "            \"test_id\": test_id,\n",
        "            \"status\": test[\"status\"],\n",
        "            \"total_requests\": test[\"total_requests\"],\n",
        "            \"variant_a\": {\n",
        "                \"version\": test[\"variant_a\"][\"version\"],\n",
        "                \"requests\": test[\"variant_a\"][\"requests\"],\n",
        "                \"percentage\": (test[\"variant_a\"][\"requests\"] / test[\"total_requests\"] * 100)\n",
        "                              if test[\"total_requests\"] > 0 else 0\n",
        "            },\n",
        "            \"variant_b\": {\n",
        "                \"version\": test[\"variant_b\"][\"version\"],\n",
        "                \"requests\": test[\"variant_b\"][\"requests\"],\n",
        "                \"percentage\": (test[\"variant_b\"][\"requests\"] / test[\"total_requests\"] * 100)\n",
        "                              if test[\"total_requests\"] > 0 else 0\n",
        "            }\n",
        "        }\n",
        "\n",
        "\n",
        "# ========================================\n",
        "# Test the A/B Testing Framework\n",
        "# ========================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üß™ Testing A/B Framework...\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "# Create A/B test manager\n",
        "ab_manager = ABTestManager()\n",
        "\n",
        "# Create a test: 80% get v1.0, 20% get v1.1\n",
        "test = ab_manager.create_ab_test(\n",
        "    test_id=\"customer_support_feb_2024\",\n",
        "    prompt_id=\"customer_support\",\n",
        "    variant_a_version=\"1.0\",  # Control (80%)\n",
        "    variant_b_version=\"1.1\",  # Treatment (20%)\n",
        "    traffic_split=0.2         # 20% to variant B\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 70)\n",
        "print(\"üìä Simulating 100 User Requests...\")\n",
        "print(\"-\" * 70 + \"\\n\")\n",
        "\n",
        "# Simulate 100 users\n",
        "for i in range(100):\n",
        "    user_id = f\"user_{i}@prudential.com\"\n",
        "    result = ab_manager.get_prompt_for_user(\"customer_support_feb_2024\", user_id)\n",
        "\n",
        "# Get statistics\n",
        "stats = ab_manager.get_test_stats(\"customer_support_feb_2024\")\n",
        "\n",
        "print(\"üìà A/B Test Results:\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"Test ID: {stats['test_id']}\")\n",
        "print(f\"Status: {stats['status']}\")\n",
        "print(f\"Total Requests: {stats['total_requests']}\")\n",
        "print()\n",
        "print(f\"Variant A (Control - v{stats['variant_a']['version']}):\")\n",
        "print(f\"  Requests: {stats['variant_a']['requests']}\")\n",
        "print(f\"  Percentage: {stats['variant_a']['percentage']:.1f}%\")\n",
        "print()\n",
        "print(f\"Variant B (Treatment - v{stats['variant_b']['version']}):\")\n",
        "print(f\"  Requests: {stats['variant_b']['requests']}\")\n",
        "print(f\"  Percentage: {stats['variant_b']['percentage']:.1f}%\")\n",
        "print()\n",
        "\n",
        "# Demonstrate consistent hashing\n",
        "print(\"-\" * 70)\n",
        "print(\"üîí Testing Consistent Hashing (same user = same variant)...\")\n",
        "print(\"-\" * 70 + \"\\n\")\n",
        "\n",
        "test_user = \"alice@prudential.com\"\n",
        "assignments = []\n",
        "\n",
        "for i in range(5):\n",
        "    result = ab_manager.assign_variant(\"customer_support_feb_2024\", test_user)\n",
        "    assignments.append(result[\"assigned_variant\"])\n",
        "\n",
        "print(f\"User: {test_user}\")\n",
        "print(f\"Assignment (5 requests): {assignments}\")\n",
        "print(f\"‚úÖ All same variant: {len(set(assignments)) == 1}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ A/B Testing Framework Complete!\")\n",
        "print(\"=\" * 70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jR3sM8JlbAC_",
        "outputId": "9008769c-6c9a-427e-f402-634a5621462e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üî¨ Building A/B Testing Framework for Prompts...\n",
            "\n",
            "\n",
            "======================================================================\n",
            "üß™ Testing A/B Framework...\n",
            "======================================================================\n",
            "\n",
            "‚úÖ A/B Test Created: customer_support_feb_2024\n",
            "   Prompt: customer_support\n",
            "   Variant A (Control): v1.0 - 80% traffic\n",
            "   Variant B (Treatment): v1.1 - 20% traffic\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "üìä Simulating 100 User Requests...\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "üìà A/B Test Results:\n",
            "----------------------------------------------------------------------\n",
            "Test ID: customer_support_feb_2024\n",
            "Status: active\n",
            "Total Requests: 100\n",
            "\n",
            "Variant A (Control - v1.0):\n",
            "  Requests: 82\n",
            "  Percentage: 82.0%\n",
            "\n",
            "Variant B (Treatment - v1.1):\n",
            "  Requests: 18\n",
            "  Percentage: 18.0%\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "üîí Testing Consistent Hashing (same user = same variant)...\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "User: alice@prudential.com\n",
            "Assignment (5 requests): ['A', 'A', 'A', 'A', 'A']\n",
            "‚úÖ All same variant: True\n",
            "\n",
            "======================================================================\n",
            "‚úÖ A/B Testing Framework Complete!\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# COMPONENT 2: Model Registry\n",
        "# ========================================\n",
        "\n",
        "print(\"ü§ñ Building Model Registry...\")\n",
        "print()\n",
        "\n",
        "# Define our approved model catalog\n",
        "model_catalog = {\n",
        "    \"gemini-1.5-pro\": {\n",
        "        \"model_id\": \"gemini-1.5-pro\",\n",
        "        \"display_name\": \"Gemini 1.5 Pro\",\n",
        "        \"provider\": \"Google\",\n",
        "        \"model_type\": \"foundation\",\n",
        "        \"status\": \"approved\",\n",
        "        \"tier\": \"premium\",\n",
        "        \"capabilities\": [\"text-generation\", \"code-generation\", \"analysis\"],\n",
        "        \"max_tokens\": 2000000,  # 2M token context window\n",
        "        \"approved_date\": \"2024-11-01\",\n",
        "        \"approved_by\": \"ML Governance Committee\"\n",
        "    },\n",
        "\n",
        "    \"gemini-1.5-flash\": {\n",
        "        \"model_id\": \"gemini-1.5-flash\",\n",
        "        \"display_name\": \"Gemini 1.5 Flash\",\n",
        "        \"provider\": \"Google\",\n",
        "        \"model_type\": \"foundation\",\n",
        "        \"status\": \"approved\",\n",
        "        \"tier\": \"standard\",\n",
        "        \"capabilities\": [\"text-generation\", \"high-volume-tasks\"],\n",
        "        \"max_tokens\": 1000000,  # 1M token context window\n",
        "        \"approved_date\": \"2024-11-01\",\n",
        "        \"approved_by\": \"ML Governance Committee\"\n",
        "    },\n",
        "\n",
        "    \"claude-3-opus\": {\n",
        "        \"model_id\": \"claude-3-opus\",\n",
        "        \"display_name\": \"Claude 3 Opus\",\n",
        "        \"provider\": \"Anthropic\",\n",
        "        \"model_type\": \"foundation\",\n",
        "        \"status\": \"approved\",\n",
        "        \"tier\": \"premium\",\n",
        "        \"capabilities\": [\"text-generation\", \"analysis\", \"long-context\"],\n",
        "        \"max_tokens\": 200000,  # 200K token context\n",
        "        \"approved_date\": \"2024-10-15\",\n",
        "        \"approved_by\": \"ML Governance Committee\"\n",
        "    },\n",
        "\n",
        "    \"customer-support-v1.2\": {\n",
        "        \"model_id\": \"customer-support-v1.2\",\n",
        "        \"display_name\": \"Customer Support Model v1.2\",\n",
        "        \"provider\": \"Prudential (Fine-tuned Gemini)\",\n",
        "        \"model_type\": \"fine-tuned\",\n",
        "        \"base_model\": \"gemini-1.5-pro\",\n",
        "        \"status\": \"approved\",\n",
        "        \"tier\": \"custom\",\n",
        "        \"capabilities\": [\"customer-service\", \"policy-questions\"],\n",
        "        \"max_tokens\": 2000000,\n",
        "        \"approved_date\": \"2024-11-15\",\n",
        "        \"approved_by\": \"ML Governance Committee\",\n",
        "        \"training_data\": \"gs://prudential-data/customer-support-conversations\",\n",
        "        \"use_case_restriction\": \"customer-support-only\"\n",
        "    },\n",
        "\n",
        "    \"gpt-4\": {\n",
        "        \"model_id\": \"gpt-4\",\n",
        "        \"display_name\": \"GPT-4\",\n",
        "        \"provider\": \"OpenAI\",\n",
        "        \"model_type\": \"foundation\",\n",
        "        \"status\": \"deprecated\",\n",
        "        \"tier\": \"premium\",\n",
        "        \"capabilities\": [\"text-generation\"],\n",
        "        \"max_tokens\": 128000,\n",
        "        \"deprecated_date\": \"2024-12-01\",\n",
        "        \"deprecated_reason\": \"Security review failed - data residency concerns\",\n",
        "        \"replacement_model\": \"gemini-1.5-pro\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save catalog to file\n",
        "catalog_file = '/content/genaiops/models/model_catalog.json'\n",
        "with open(catalog_file, 'w') as f:\n",
        "    json.dump(model_catalog, f, indent=2)\n",
        "\n",
        "print(f\"‚úÖ Model catalog created with {len(model_catalog)} models\")\n",
        "print()\n",
        "\n",
        "# Display summary\n",
        "print(\"üìä Model Summary:\")\n",
        "print(\"-\" * 60)\n",
        "for model_id, details in model_catalog.items():\n",
        "    status_emoji = \"‚úÖ\" if details[\"status\"] == \"approved\" else \"‚ö†Ô∏è\" if details[\"status\"] == \"testing\" else \"‚ùå\"\n",
        "    print(f\"{status_emoji} {details['display_name']}\")\n",
        "    print(f\"   Status: {details['status']} | Provider: {details['provider']} | Tier: {details['tier']}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMPTtkbcxRIy",
        "outputId": "95d389f5-c792-4ef5-96ee-15214b93a0f1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü§ñ Building Model Registry...\n",
            "\n",
            "‚úÖ Model catalog created with 5 models\n",
            "\n",
            "üìä Model Summary:\n",
            "------------------------------------------------------------\n",
            "‚úÖ Gemini 1.5 Pro\n",
            "   Status: approved | Provider: Google | Tier: premium\n",
            "\n",
            "‚úÖ Gemini 1.5 Flash\n",
            "   Status: approved | Provider: Google | Tier: standard\n",
            "\n",
            "‚úÖ Claude 3 Opus\n",
            "   Status: approved | Provider: Anthropic | Tier: premium\n",
            "\n",
            "‚úÖ Customer Support Model v1.2\n",
            "   Status: approved | Provider: Prudential (Fine-tuned Gemini) | Tier: custom\n",
            "\n",
            "‚ùå GPT-4\n",
            "   Status: deprecated | Provider: OpenAI | Tier: premium\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# Add Cost Information to Model Registry\n",
        "# ========================================\n",
        "\n",
        "print(\"üí∞ Adding cost tracking to Model Registry...\")\n",
        "print()\n",
        "\n",
        "# Cost data for each model (pricing per 1K tokens)\n",
        "model_costs = {\n",
        "    \"gemini-1.5-pro\": {\n",
        "        \"input_cost_per_1k\": 0.00125,   # $0.00125 per 1K input tokens\n",
        "        \"output_cost_per_1k\": 0.005,    # $0.005 per 1K output tokens\n",
        "        \"cost_tier\": \"premium\",\n",
        "        \"notes\": \"Best quality, highest cost\"\n",
        "    },\n",
        "\n",
        "    \"gemini-1.5-flash\": {\n",
        "        \"input_cost_per_1k\": 0.000075,  # $0.000075 per 1K input tokens\n",
        "        \"output_cost_per_1k\": 0.0003,   # $0.0003 per 1K output tokens\n",
        "        \"cost_tier\": \"budget\",\n",
        "        \"notes\": \"Fast and cheap, good for high-volume\"\n",
        "    },\n",
        "\n",
        "    \"claude-3-opus\": {\n",
        "        \"input_cost_per_1k\": 0.015,     # $0.015 per 1K input tokens\n",
        "        \"output_cost_per_1k\": 0.075,    # $0.075 per 1K output tokens\n",
        "        \"cost_tier\": \"premium-plus\",\n",
        "        \"notes\": \"Most expensive, best for complex tasks\"\n",
        "    },\n",
        "\n",
        "    \"customer-support-v1.2\": {\n",
        "        \"input_cost_per_1k\": 0.00125,   # Same as base Gemini Pro\n",
        "        \"output_cost_per_1k\": 0.005,\n",
        "        \"cost_tier\": \"premium\",\n",
        "        \"training_cost\": 850.00,        # One-time training cost\n",
        "        \"notes\": \"Fine-tuned model, training cost already paid\"\n",
        "    },\n",
        "\n",
        "    \"gpt-4\": {\n",
        "        \"input_cost_per_1k\": 0.03,      # Expensive (deprecated)\n",
        "        \"output_cost_per_1k\": 0.06,\n",
        "        \"cost_tier\": \"deprecated\",\n",
        "        \"notes\": \"Deprecated - do not use\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save cost data\n",
        "cost_file = '/content/genaiops/models/model_costs.json'\n",
        "with open(cost_file, 'w') as f:\n",
        "    json.dump(model_costs, f, indent=2)\n",
        "\n",
        "print(\"‚úÖ Cost tracking added for all models\")\n",
        "print()\n",
        "\n",
        "# Display cost comparison\n",
        "print(\"üíµ Cost Comparison (per 1K tokens):\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"{'Model':<30} {'Input Cost':>12} {'Output Cost':>12} {'Tier':<15}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for model_id, costs in model_costs.items():\n",
        "    if model_id in model_catalog and model_catalog[model_id][\"status\"] != \"deprecated\":\n",
        "        input_cost = f\"${costs['input_cost_per_1k']:.6f}\"\n",
        "        output_cost = f\"${costs['output_cost_per_1k']:.6f}\"\n",
        "        tier = costs['cost_tier']\n",
        "\n",
        "        model_name = model_catalog[model_id][\"display_name\"]\n",
        "        print(f\"{model_name:<30} {input_cost:>12} {output_cost:>12} {tier:<15}\")\n",
        "\n",
        "print()\n",
        "\n",
        "# Calculate example: 1 million tokens\n",
        "print(\"üìä Example Cost Calculation:\")\n",
        "print(\"Scenario: Process 1M input tokens + generate 500K output tokens\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "example_input_tokens = 1000000  # 1M tokens\n",
        "example_output_tokens = 500000  # 500K tokens\n",
        "\n",
        "for model_id in [\"gemini-1.5-pro\", \"gemini-1.5-flash\", \"claude-3-opus\"]:\n",
        "    if model_id in model_costs:\n",
        "        costs = model_costs[model_id]\n",
        "\n",
        "        # Calculate cost\n",
        "        input_cost = (example_input_tokens / 1000) * costs['input_cost_per_1k']\n",
        "        output_cost = (example_output_tokens / 1000) * costs['output_cost_per_1k']\n",
        "        total_cost = input_cost + output_cost\n",
        "\n",
        "        model_name = model_catalog[model_id][\"display_name\"]\n",
        "        print(f\"{model_name:<30} Total Cost: ${total_cost:,.2f}\")\n",
        "\n",
        "print()\n",
        "print(\"üí° Insight: Gemini Flash is 94% cheaper than Claude Opus for high-volume tasks!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVg_Xcw4IiMI",
        "outputId": "b95145d3-bc02-435b-9221-c660e152ae60"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üí∞ Adding cost tracking to Model Registry...\n",
            "\n",
            "‚úÖ Cost tracking added for all models\n",
            "\n",
            "üíµ Cost Comparison (per 1K tokens):\n",
            "--------------------------------------------------------------------------------\n",
            "Model                            Input Cost  Output Cost Tier           \n",
            "--------------------------------------------------------------------------------\n",
            "Gemini 1.5 Pro                    $0.001250    $0.005000 premium        \n",
            "Gemini 1.5 Flash                  $0.000075    $0.000300 budget         \n",
            "Claude 3 Opus                     $0.015000    $0.075000 premium-plus   \n",
            "Customer Support Model v1.2       $0.001250    $0.005000 premium        \n",
            "\n",
            "üìä Example Cost Calculation:\n",
            "Scenario: Process 1M input tokens + generate 500K output tokens\n",
            "--------------------------------------------------------------------------------\n",
            "Gemini 1.5 Pro                 Total Cost: $3.75\n",
            "Gemini 1.5 Flash               Total Cost: $0.22\n",
            "Claude 3 Opus                  Total Cost: $52.50\n",
            "\n",
            "üí° Insight: Gemini Flash is 94% cheaper than Claude Opus for high-volume tasks!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# Model Loader Function\n",
        "# ========================================\n",
        "\n",
        "def load_model_info(model_id):\n",
        "    \"\"\"\n",
        "    Load model information from registry\n",
        "\n",
        "    Args:\n",
        "        model_id: ID of the model (e.g., 'gemini-1.5-pro')\n",
        "\n",
        "    Returns:\n",
        "        dict with model details, costs, and approval status\n",
        "    \"\"\"\n",
        "\n",
        "    # Check if model exists in catalog\n",
        "    if model_id not in model_catalog:\n",
        "        return {\n",
        "            \"error\": f\"Model '{model_id}' not found in registry\",\n",
        "            \"available_models\": list(model_catalog.keys())\n",
        "        }\n",
        "\n",
        "    # Get model details\n",
        "    model_details = model_catalog[model_id]\n",
        "\n",
        "    # Check if model is approved\n",
        "    if model_details[\"status\"] == \"deprecated\":\n",
        "        return {\n",
        "            \"error\": f\"Model '{model_id}' is deprecated\",\n",
        "            \"status\": \"deprecated\",\n",
        "            \"deprecated_reason\": model_details.get(\"deprecated_reason\", \"Unknown\"),\n",
        "            \"replacement\": model_details.get(\"replacement_model\", \"Contact ML team\")\n",
        "        }\n",
        "\n",
        "    if model_details[\"status\"] != \"approved\":\n",
        "        return {\n",
        "            \"error\": f\"Model '{model_id}' is not approved for use\",\n",
        "            \"status\": model_details[\"status\"],\n",
        "            \"message\": \"Only approved models can be used in production\"\n",
        "        }\n",
        "\n",
        "    # Get cost information\n",
        "    cost_info = model_costs.get(model_id, {\n",
        "        \"input_cost_per_1k\": \"Unknown\",\n",
        "        \"output_cost_per_1k\": \"Unknown\",\n",
        "        \"cost_tier\": \"Unknown\"\n",
        "    })\n",
        "\n",
        "    # Combine all information\n",
        "    return {\n",
        "        \"model_id\": model_id,\n",
        "        \"details\": model_details,\n",
        "        \"costs\": cost_info,\n",
        "        \"status\": \"ready\",\n",
        "        \"message\": f\"‚úÖ {model_details['display_name']} is approved and ready to use\"\n",
        "    }\n",
        "\n",
        "\n",
        "# ========================================\n",
        "# Test the Model Loader\n",
        "# ========================================\n",
        "\n",
        "print(\"üß™ Testing Model Loader Function...\")\n",
        "print()\n",
        "\n",
        "# Test 1: Load approved model\n",
        "print(\"Test 1: Load Gemini 1.5 Flash (approved)\")\n",
        "print(\"-\" * 60)\n",
        "result1 = load_model_info(\"gemini-1.5-flash\")\n",
        "\n",
        "if \"error\" not in result1:\n",
        "    print(f\"‚úÖ {result1['message']}\")\n",
        "    print(f\"   Provider: {result1['details']['provider']}\")\n",
        "    print(f\"   Tier: {result1['details']['tier']}\")\n",
        "    print(f\"   Input Cost: ${result1['costs']['input_cost_per_1k']:.6f} per 1K tokens\")\n",
        "    print(f\"   Output Cost: ${result1['costs']['output_cost_per_1k']:.6f} per 1K tokens\")\n",
        "else:\n",
        "    print(f\"‚ùå {result1['error']}\")\n",
        "\n",
        "print()\n",
        "\n",
        "# Test 2: Try to load deprecated model\n",
        "print(\"Test 2: Try to load GPT-4 (deprecated)\")\n",
        "print(\"-\" * 60)\n",
        "result2 = load_model_info(\"gpt-4\")\n",
        "\n",
        "if \"error\" in result2:\n",
        "    print(f\"‚ùå {result2['error']}\")\n",
        "    print(f\"   Reason: {result2.get('deprecated_reason', 'N/A')}\")\n",
        "    print(f\"   Use instead: {result2.get('replacement', 'N/A')}\")\n",
        "else:\n",
        "    print(f\"‚úÖ Model loaded\")\n",
        "\n",
        "print()\n",
        "\n",
        "# Test 3: Try to load non-existent model\n",
        "print(\"Test 3: Try to load non-existent model\")\n",
        "print(\"-\" * 60)\n",
        "result3 = load_model_info(\"gpt-5-turbo\")\n",
        "\n",
        "if \"error\" in result3:\n",
        "    print(f\"‚ùå {result3['error']}\")\n",
        "    print(f\"   Available models: {', '.join(result3['available_models'][:3])}...\")\n",
        "else:\n",
        "    print(f\"‚úÖ Model loaded\")\n",
        "\n",
        "print()\n",
        "print(\"=\" * 60)\n",
        "print(\"‚úÖ Model Loader function is working correctly!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gpant5VtNj6D",
        "outputId": "5d968509-03da-4402-bf83-c68e612484fe"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ Testing Model Loader Function...\n",
            "\n",
            "Test 1: Load Gemini 1.5 Flash (approved)\n",
            "------------------------------------------------------------\n",
            "‚úÖ ‚úÖ Gemini 1.5 Flash is approved and ready to use\n",
            "   Provider: Google\n",
            "   Tier: standard\n",
            "   Input Cost: $0.000075 per 1K tokens\n",
            "   Output Cost: $0.000300 per 1K tokens\n",
            "\n",
            "Test 2: Try to load GPT-4 (deprecated)\n",
            "------------------------------------------------------------\n",
            "‚ùå Model 'gpt-4' is deprecated\n",
            "   Reason: Security review failed - data residency concerns\n",
            "   Use instead: gemini-1.5-pro\n",
            "\n",
            "Test 3: Try to load non-existent model\n",
            "------------------------------------------------------------\n",
            "‚ùå Model 'gpt-5-turbo' not found in registry\n",
            "   Available models: gemini-1.5-pro, gemini-1.5-flash, claude-3-opus...\n",
            "\n",
            "============================================================\n",
            "‚úÖ Model Loader function is working correctly!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# Cost Calculator Function\n",
        "# ========================================\n",
        "\n",
        "def calculate_cost(model_id, input_tokens, output_tokens):\n",
        "    \"\"\"\n",
        "    Calculate cost for using a specific model\n",
        "\n",
        "    Args:\n",
        "        model_id: ID of the model\n",
        "        input_tokens: Number of input tokens\n",
        "        output_tokens: Number of output tokens\n",
        "\n",
        "    Returns:\n",
        "        dict with cost breakdown\n",
        "    \"\"\"\n",
        "\n",
        "    # Load model info first (includes validation)\n",
        "    model_info = load_model_info(model_id)\n",
        "\n",
        "    # Check if model can be used\n",
        "    if \"error\" in model_info:\n",
        "        return {\n",
        "            \"error\": model_info[\"error\"],\n",
        "            \"suggestion\": model_info.get(\"replacement\", \"Choose an approved model\")\n",
        "        }\n",
        "\n",
        "    # Get costs\n",
        "    costs = model_info[\"costs\"]\n",
        "\n",
        "    # Calculate\n",
        "    input_cost = (input_tokens / 1000) * costs[\"input_cost_per_1k\"]\n",
        "    output_cost = (output_tokens / 1000) * costs[\"output_cost_per_1k\"]\n",
        "    total_cost = input_cost + output_cost\n",
        "\n",
        "    return {\n",
        "        \"model_id\": model_id,\n",
        "        \"model_name\": model_info[\"details\"][\"display_name\"],\n",
        "        \"breakdown\": {\n",
        "            \"input_tokens\": input_tokens,\n",
        "            \"input_cost\": input_cost,\n",
        "            \"output_tokens\": output_tokens,\n",
        "            \"output_cost\": output_cost,\n",
        "            \"total_cost\": total_cost\n",
        "        },\n",
        "        \"formatted\": f\"${total_cost:.4f}\",\n",
        "        \"cost_tier\": costs[\"cost_tier\"]\n",
        "    }\n",
        "\n",
        "\n",
        "def compare_model_costs(input_tokens, output_tokens, models=None):\n",
        "    \"\"\"\n",
        "    Compare costs across multiple models\n",
        "\n",
        "    Args:\n",
        "        input_tokens: Number of input tokens\n",
        "        output_tokens: Number of output tokens\n",
        "        models: List of model IDs to compare (default: all approved)\n",
        "\n",
        "    Returns:\n",
        "        list of cost comparisons, sorted by price\n",
        "    \"\"\"\n",
        "\n",
        "    # Default to all approved models\n",
        "    if models is None:\n",
        "        models = [\n",
        "            model_id for model_id, details in model_catalog.items()\n",
        "            if details[\"status\"] == \"approved\"\n",
        "        ]\n",
        "\n",
        "    # Calculate cost for each model\n",
        "    comparisons = []\n",
        "    for model_id in models:\n",
        "        result = calculate_cost(model_id, input_tokens, output_tokens)\n",
        "        if \"error\" not in result:\n",
        "            comparisons.append(result)\n",
        "\n",
        "    # Sort by cost (cheapest first)\n",
        "    comparisons.sort(key=lambda x: x[\"breakdown\"][\"total_cost\"])\n",
        "\n",
        "    return comparisons\n",
        "\n",
        "\n",
        "# ========================================\n",
        "# Test Cost Calculator\n",
        "# ========================================\n",
        "\n",
        "print(\"üß™ Testing Cost Calculator...\")\n",
        "print()\n",
        "\n",
        "# Test 1: Calculate cost for single model\n",
        "print(\"Test 1: Cost for 10,000 customer support messages\")\n",
        "print(\"Assumptions: 100 input tokens + 150 output tokens per message\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "messages = 10000\n",
        "input_per_message = 100\n",
        "output_per_message = 150\n",
        "\n",
        "total_input = messages * input_per_message   # 1M tokens\n",
        "total_output = messages * output_per_message # 1.5M tokens\n",
        "\n",
        "result = calculate_cost(\"gemini-1.5-flash\", total_input, total_output)\n",
        "\n",
        "if \"error\" not in result:\n",
        "    print(f\"Model: {result['model_name']}\")\n",
        "    print(f\"  Input: {result['breakdown']['input_tokens']:,} tokens ‚Üí ${result['breakdown']['input_cost']:.2f}\")\n",
        "    print(f\"  Output: {result['breakdown']['output_tokens']:,} tokens ‚Üí ${result['breakdown']['output_cost']:.2f}\")\n",
        "    print(f\"  Total Cost: ${result['breakdown']['total_cost']:.2f}\")\n",
        "    print(f\"  Cost per message: ${result['breakdown']['total_cost'] / messages:.4f}\")\n",
        "\n",
        "print()\n",
        "\n",
        "# Test 2: Compare all approved models\n",
        "print(\"Test 2: Compare costs across all approved models\")\n",
        "print(f\"Scenario: {total_input:,} input tokens + {total_output:,} output tokens\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "comparisons = compare_model_costs(total_input, total_output)\n",
        "\n",
        "print(f\"{'Model':<30} {'Total Cost':>12} {'Cost Tier':<15}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for i, comp in enumerate(comparisons, 1):\n",
        "    model_name = comp[\"model_name\"]\n",
        "    total_cost = comp[\"breakdown\"][\"total_cost\"]\n",
        "    tier = comp[\"cost_tier\"]\n",
        "\n",
        "    rank_emoji = \"ü•á\" if i == 1 else \"ü•à\" if i == 2 else \"ü•â\" if i == 3 else \"  \"\n",
        "    print(f\"{rank_emoji} {model_name:<28} ${total_cost:>10,.2f} {tier:<15}\")\n",
        "\n",
        "print()\n",
        "\n",
        "# Calculate savings\n",
        "if len(comparisons) > 1:\n",
        "    cheapest = comparisons[0][\"breakdown\"][\"total_cost\"]\n",
        "    most_expensive = comparisons[-1][\"breakdown\"][\"total_cost\"]\n",
        "    savings = most_expensive - cheapest\n",
        "    savings_percent = (savings / most_expensive) * 100\n",
        "\n",
        "    print(f\"üí° Insight:\")\n",
        "    print(f\"   Cheapest: {comparisons[0]['model_name']} (${cheapest:,.2f})\")\n",
        "    print(f\"   Most expensive: {comparisons[-1]['model_name']} (${most_expensive:,.2f})\")\n",
        "    print(f\"   Potential savings: ${savings:,.2f} ({savings_percent:.1f}% cheaper)\")\n",
        "\n",
        "print()\n",
        "\n",
        "# Test 3: Budget planning\n",
        "print(\"Test 3: Budget Planning - What can I afford?\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "monthly_budget = 1000  # $1,000/month\n",
        "messages_per_month = 50000\n",
        "\n",
        "input_per_msg = 100\n",
        "output_per_msg = 150\n",
        "\n",
        "total_input_monthly = messages_per_month * input_per_msg\n",
        "total_output_monthly = messages_per_month * output_per_msg\n",
        "\n",
        "print(f\"Budget: ${monthly_budget}/month\")\n",
        "print(f\"Expected volume: {messages_per_month:,} messages/month\")\n",
        "print(f\"Tokens per message: {input_per_msg} input + {output_per_msg} output\")\n",
        "print()\n",
        "\n",
        "comparisons = compare_model_costs(total_input_monthly, total_output_monthly)\n",
        "\n",
        "print(f\"{'Model':<30} {'Monthly Cost':>12} {'Within Budget?':<15}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for comp in comparisons:\n",
        "    model_name = comp[\"model_name\"]\n",
        "    monthly_cost = comp[\"breakdown\"][\"total_cost\"]\n",
        "\n",
        "    within_budget = monthly_cost <= monthly_budget\n",
        "    status = \"‚úÖ Yes\" if within_budget else \"‚ùå Over budget\"\n",
        "\n",
        "    print(f\"{model_name:<30} ${monthly_cost:>10,.2f} {status:<15}\")\n",
        "\n",
        "print()\n",
        "print(\"=\" * 70)\n",
        "print(\"‚úÖ Cost Calculator is working!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2cMbYDRQzHb",
        "outputId": "ad55d271-7e6c-4d59-d124-f76075e5698b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ Testing Cost Calculator...\n",
            "\n",
            "Test 1: Cost for 10,000 customer support messages\n",
            "Assumptions: 100 input tokens + 150 output tokens per message\n",
            "----------------------------------------------------------------------\n",
            "Model: Gemini 1.5 Flash\n",
            "  Input: 1,000,000 tokens ‚Üí $0.07\n",
            "  Output: 1,500,000 tokens ‚Üí $0.45\n",
            "  Total Cost: $0.52\n",
            "  Cost per message: $0.0001\n",
            "\n",
            "Test 2: Compare costs across all approved models\n",
            "Scenario: 1,000,000 input tokens + 1,500,000 output tokens\n",
            "----------------------------------------------------------------------\n",
            "Model                            Total Cost Cost Tier      \n",
            "----------------------------------------------------------------------\n",
            "ü•á Gemini 1.5 Flash             $      0.52 budget         \n",
            "ü•à Gemini 1.5 Pro               $      8.75 premium        \n",
            "ü•â Customer Support Model v1.2  $      8.75 premium        \n",
            "   Claude 3 Opus                $    127.50 premium-plus   \n",
            "\n",
            "üí° Insight:\n",
            "   Cheapest: Gemini 1.5 Flash ($0.52)\n",
            "   Most expensive: Claude 3 Opus ($127.50)\n",
            "   Potential savings: $126.97 (99.6% cheaper)\n",
            "\n",
            "Test 3: Budget Planning - What can I afford?\n",
            "----------------------------------------------------------------------\n",
            "Budget: $1000/month\n",
            "Expected volume: 50,000 messages/month\n",
            "Tokens per message: 100 input + 150 output\n",
            "\n",
            "Model                          Monthly Cost Within Budget? \n",
            "----------------------------------------------------------------------\n",
            "Gemini 1.5 Flash               $      2.62 ‚úÖ Yes          \n",
            "Gemini 1.5 Pro                 $     43.75 ‚úÖ Yes          \n",
            "Customer Support Model v1.2    $     43.75 ‚úÖ Yes          \n",
            "Claude 3 Opus                  $    637.50 ‚úÖ Yes          \n",
            "\n",
            "======================================================================\n",
            "‚úÖ Cost Calculator is working!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-nOYKsUhU1l6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}