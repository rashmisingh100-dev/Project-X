{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbJGj4nhf6GRifjJKllMq4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rashmisingh100-dev/Project-X/blob/main/GenAIOps_Framework_Module1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFavuHwW2qXp",
        "outputId": "792ce50d-2ebe-4459-c94d-3f87cb59231c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GenAIOps Framework - Starting Setup\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "#GenAI Ops Framework - Module 1: Foundation\n",
        "#This module builds the core GenAI components\n",
        "print(\"GenAIOps Framework - Starting Setup\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create Visual Directory Setup\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "Aaugcm203HlT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Setup directories\n",
        "base_dir= Path('/content/genaiops')\n",
        "base_dir.mkdir(exist_ok=True)"
      ],
      "metadata": {
        "id": "XyDJvnhe3Csp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create subsidirectories\n",
        "(base_dir/'prompts').mkdir(exist_ok=True)\n",
        "(base_dir / 'models').mkdir(exist_ok=True)\n",
        "(base_dir / 'evaluations').mkdir(exist_ok=True)\n",
        "(base_dir / 'logs').mkdir(exist_ok=True)"
      ],
      "metadata": {
        "id": "_-ISBxDc36Nj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"‚úÖ Environment ready!\")\n",
        "print(f\"üìÅ Base directory: {base_dir}\")\n",
        "print()\n",
        "print(\"üëâ You can now run the rest of the notebook\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_7XCAwW3jLH",
        "outputId": "d8672be3-6778-4f81-b35a-1d04691be143"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Environment ready!\n",
            "üìÅ Base directory: /content/genaiops\n",
            "\n",
            "üëâ You can now run the rest of the notebook\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#verify setup worked\n",
        "import os\n",
        "print(\"üîç Verifying GenAIOps directory structure...\")\n",
        "print()\n",
        "\n",
        "for folder in ['prompts', 'models', 'evaluations', 'logs']:\n",
        "    path = f'/content/genaiops/{folder}'\n",
        "    exists = os.path.exists(path)\n",
        "    status = \"‚úÖ\" if exists else \"‚ùå\"\n",
        "    print(f\"{status} {path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3ZmVuNXAXc-",
        "outputId": "897b047b-a701-48ce-b379-8162c99e06f1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Verifying GenAIOps directory structure...\n",
            "\n",
            "‚úÖ /content/genaiops/prompts\n",
            "‚úÖ /content/genaiops/models\n",
            "‚úÖ /content/genaiops/evaluations\n",
            "‚úÖ /content/genaiops/logs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# COMPONENT 1: Prompt Management System\n",
        "# ========================================\n",
        "print(\"üìù Building Prompt Management System...\")\n",
        "print()\n",
        "#Define Prompt Template for Customer Support\n",
        "customer_support_prompt_v1 = \"\"\"\n",
        "You are a helpful customer service representative for Prudential Financial.\n",
        "\n",
        "Customer Question:{customer_question}\n",
        "\n",
        "Instructions:\n",
        "- Be professional and empathetic\n",
        "- Provide accurate information about policies, only factual and grounded answer with no hallucination\n",
        "- If you don't know the answer, say so clearly\n",
        "- Keep response under 150 words\n",
        "- Include next steps when applicable\n",
        "- Professional yet conversational tone\n",
        "- Include 2-3 specific next steps\n",
        "- Offer specialist escalation if complex\n",
        "\n",
        "Safety Rules:\n",
        "- Never provide medical advice\n",
        "- Never make financial predictions\n",
        "- Don't discuss other customers\n",
        "- Escalate legal questions to compliance team\n",
        "Response:\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpfeMYoEAvBE",
        "outputId": "4731bd2e-42e5-43a9-8a54-e311b341653f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìù Building Prompt Management System...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save this prompt to our prompts directory\n",
        "prompt_file_path = '/content/genaiops/prompts/customer_support_v1.0.txt'\n",
        "\n",
        "with open(prompt_file_path, 'w') as f:\n",
        "    f.write(customer_support_prompt_v1)\n",
        "\n",
        "print(f\"‚úÖ Prompt saved to: {prompt_file_path}\")\n",
        "print()\n",
        "print(\"üìÑ Prompt content:\")\n",
        "print(\"-\" * 50)\n",
        "print(customer_support_prompt_v1)\n"
      ],
      "metadata": {
        "id": "9cf-fKg8FgHE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcba7b02-7b6b-4330-abd2-2d3046a2641d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Prompt saved to: /content/genaiops/prompts/customer_support_v1.0.txt\n",
            "\n",
            "üìÑ Prompt content:\n",
            "--------------------------------------------------\n",
            "\n",
            "You are a helpful customer service representative for Prudential Financial.\n",
            "\n",
            "Customer Question:{customer_question}\n",
            "\n",
            "Instructions:\n",
            "- Be professional and empathetic\n",
            "- Provide accurate information about policies, only factual and grounded answer with no hallucination\n",
            "- If you don't know the answer, say so clearly\n",
            "- Keep response under 150 words\n",
            "- Include next steps when applicable\n",
            "- Professional yet conversational tone\n",
            "- Include 2-3 specific next steps\n",
            "- Offer specialist escalation if complex\n",
            "\n",
            "Safety Rules:\n",
            "- Never provide medical advice\n",
            "- Never make financial predictions\n",
            "- Don't discuss other customers\n",
            "- Escalate legal questions to compliance team\n",
            "Response:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prompt Metadata (Governance)\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# Create metadata for our prompt\n",
        "prompt_metadata = {\n",
        "    \"prompt_id\": \"customer_support_v1.0\",\n",
        "    \"version\": \"1.0\",\n",
        "    \"created_date\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
        "    \"created_by\": \"Rashmi Singh\",\n",
        "    \"status\": \"approved\",\n",
        "    \"use_case\": \"Customer service chatbot\",\n",
        "    \"model_compatibility\": [\"gemini-1.5-pro\", \"gemini-1.5-flash\"],\n",
        "    \"approved_by\": \"Data & AI COE (Group)\",\n",
        "    \"approval_date\": \"2024-02-14\",\n",
        "    \"description\": \"Professional customer service prompt with empathy and accuracy focus\",\n",
        "    \"test_pass_rate\": 0.95,  # 95% of test cases passed\n",
        "    \"production_apps\": [\"CustomerSupportBot\", \"EmailAutomation\"]\n",
        "}\n",
        "\n",
        "# Save metadata as JSON\n",
        "metadata_file = '/content/genaiops/prompts/customer_support_v1.0_metadata.json'\n",
        "\n",
        "with open(metadata_file, 'w') as f:\n",
        "    json.dump(prompt_metadata, f, indent=2)\n",
        "\n",
        "print(\"‚úÖ Prompt metadata saved\")\n",
        "print()\n",
        "print(\"üìã Metadata:\")\n",
        "print(json.dumps(prompt_metadata, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FLL4wkWmlYK",
        "outputId": "82785636-0814-4dbf-a39f-5b087d3d3f81"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Prompt metadata saved\n",
            "\n",
            "üìã Metadata:\n",
            "{\n",
            "  \"prompt_id\": \"customer_support_v1.0\",\n",
            "  \"version\": \"1.0\",\n",
            "  \"created_date\": \"2026-02-15\",\n",
            "  \"created_by\": \"Rashmi Singh\",\n",
            "  \"status\": \"approved\",\n",
            "  \"use_case\": \"Customer service chatbot\",\n",
            "  \"model_compatibility\": [\n",
            "    \"gemini-1.5-pro\",\n",
            "    \"gemini-1.5-flash\"\n",
            "  ],\n",
            "  \"approved_by\": \"Data & AI COE (Group)\",\n",
            "  \"approval_date\": \"2024-02-14\",\n",
            "  \"description\": \"Professional customer service prompt with empathy and accuracy focus\",\n",
            "  \"test_pass_rate\": 0.95,\n",
            "  \"production_apps\": [\n",
            "    \"CustomerSupportBot\",\n",
            "    \"EmailAutomation\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prompt Loader Function\n",
        "\n",
        "def load_prompt(prompt_id, version=\"latest\"):\n",
        "    \"\"\"\n",
        "    Load a prompt template by ID and version\n",
        "\n",
        "    Args:\n",
        "        prompt_id: Name of the prompt (e.g., 'customer_support')\n",
        "        version: Version number (e.g., '1.0') or 'latest'\n",
        "\n",
        "    Returns:\n",
        "        dict with 'template' and 'metadata'\n",
        "    \"\"\"\n",
        "\n",
        "    # Construct file paths\n",
        "    if version == \"latest\":\n",
        "        # In real system, would query database for latest version\n",
        "        # For now, we'll use v1.0\n",
        "        version = \"1.0\"\n",
        "\n",
        "    prompt_file = f'/content/genaiops/prompts/{prompt_id}_v{version}.txt'\n",
        "    metadata_file = f'/content/genaiops/prompts/{prompt_id}_v{version}_metadata.json'\n",
        "\n",
        "    # Load prompt template\n",
        "    try:\n",
        "        with open(prompt_file, 'r') as f:\n",
        "            template = f.read()\n",
        "    except FileNotFoundError:\n",
        "        return {\"error\": f\"Prompt {prompt_id} v{version} not found\"}\n",
        "\n",
        "    # Load metadata\n",
        "    try:\n",
        "        with open(metadata_file, 'r') as f:\n",
        "            metadata = json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        metadata = {\"warning\": \"No metadata found\"}\n",
        "\n",
        "    return {\n",
        "        \"template\": template,\n",
        "        \"metadata\": metadata\n",
        "    }\n",
        "\n",
        "\n",
        "# Test the loader\n",
        "print(\"üß™ Testing prompt loader...\")\n",
        "print()\n",
        "\n",
        "result = load_prompt(\"customer_support\", version=\"1.0\")\n",
        "\n",
        "print(\"‚úÖ Prompt loaded successfully!\")\n",
        "print()\n",
        "print(\"üìÑ Template:\")\n",
        "print(result['template'][:200] + \"...\")  # First 200 chars\n",
        "print()\n",
        "print(\"üìã Metadata:\")\n",
        "print(f\"  Version: {result['metadata']['version']}\")\n",
        "print(f\"  Status: {result['metadata']['status']}\")\n",
        "print(f\"  Use Case: {result['metadata']['use_case']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4w5KD_4xq2Wf",
        "outputId": "6353ac7a-795d-43e1-9363-dbd3f8b0fb1b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ Testing prompt loader...\n",
            "\n",
            "‚úÖ Prompt loaded successfully!\n",
            "\n",
            "üìÑ Template:\n",
            "\n",
            "You are a helpful customer service representative for Prudential Financial.\n",
            "\n",
            "Customer Question:{customer_question}\n",
            "\n",
            "Instructions:\n",
            "- Be professional and empathetic\n",
            "- Provide accurate information about...\n",
            "\n",
            "üìã Metadata:\n",
            "  Version: 1.0\n",
            "  Status: approved\n",
            "  Use Case: Customer service chatbot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prompt Version Comparison Tool\n",
        "# Create an improved version (v1.1)\n",
        "customer_support_prompt_v1_1 = \"\"\"\n",
        "You are an empathetic customer service representative for Prudential Financial with deep knowledge of our insurance products and policies.\n",
        "\n",
        "Customer Profile:\n",
        "- Name: {customer_name}\n",
        "- Policy Type: {policy_type}\n",
        "- Customer Since: {customer_since}\n",
        "\n",
        "Customer Question:\n",
        "{customer_question}\n",
        "\n",
        "Instructions:\n",
        "- Address customer by name to personalize the response\n",
        "- Be professional, empathetic, and solution-oriented\n",
        "- Reference their specific policy type when relevant\n",
        "- Provide accurate information about Prudential policies\n",
        "- If you don't know the answer, be honest and offer to connect them with a specialist\n",
        "- Keep response under 150 words\n",
        "- Always include clear next steps\n",
        "- End with \"Is there anything else I can help you with today?\"\n",
        "- Professional yet conversational tone\n",
        "- Include 2-3 specific next steps\n",
        "- Offer specialist escalation if complex\n",
        "\n",
        "Safety Rules:\n",
        "- Never provide medical advice\n",
        "- Never make financial predictions\n",
        "- Don't discuss other customers\n",
        "- Escalate legal questions to compliance team\n",
        "\n",
        "Response:\n",
        "\"\"\"\n",
        "\n",
        "# Save v1.1\n",
        "prompt_v1_1_path = '/content/genaiops/prompts/customer_support_v1.1.txt'\n",
        "with open(prompt_v1_1_path, 'w') as f:\n",
        "    f.write(customer_support_prompt_v1_1)\n",
        "\n",
        "# Create metadata for v1.1\n",
        "metadata_v1_1 = {\n",
        "    \"prompt_id\": \"customer_support_v1.1\",\n",
        "    \"version\": \"1.1\",\n",
        "    \"created_date\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
        "    \"created_by\": \"Rashmi Singh\",\n",
        "    \"status\": \"testing\",  # Not yet approved for production\n",
        "    \"use_case\": \"Customer service chatbot\",\n",
        "    \"model_compatibility\": [\"gemini-1.5-pro\", \"gemini-1.5-flash\"],\n",
        "    \"description\": \"Enhanced with personalization and policy-type awareness\",\n",
        "    \"improvements_over_v1.0\": [\n",
        "        \"Personalization with customer name\",\n",
        "        \"Policy-type specific responses\",\n",
        "        \"Customer tenure awareness\",\n",
        "        \"Standardized closing question\"\n",
        "    ],\n",
        "    \"test_pass_rate\": None,  # Not yet tested\n",
        "    \"production_apps\": []  # Not yet deployed\n",
        "}\n",
        "\n",
        "metadata_v1_1_path = '/content/genaiops/prompts/customer_support_v1.1_metadata.json'\n",
        "with open(metadata_v1_1_path, 'w') as f:\n",
        "    json.dump(metadata_v1_1, f, indent=2)\n",
        "\n",
        "print(\"‚úÖ Created prompt v1.1 (improved version)\")\n",
        "print()\n",
        "print(\"üÜö Comparing v1.0 vs v1.1:\")\n",
        "print(\"-\" * 60)\n",
        "print(\"v1.0 (Production):\")\n",
        "print(\"  - Generic customer addressing\")\n",
        "print(\"  - No personalization\")\n",
        "print(\"  - Status: Approved ‚úÖ\")\n",
        "print()\n",
        "print(\"v1.1 (Testing):\")\n",
        "print(\"  - Personalized with customer name\")\n",
        "print(\"  - Policy-type aware\")\n",
        "print(\"  - Customer tenure aware\")\n",
        "print(\"  - Standardized closing\")\n",
        "print(\"  - Status: Testing üß™\")\n",
        "print()\n",
        "print(\"üìä Next step: A/B testing to compare quality\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NWdsm_grCZO",
        "outputId": "d64528e6-062d-406c-977c-91c16c0d2af3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Created prompt v1.1 (improved version)\n",
            "\n",
            "üÜö Comparing v1.0 vs v1.1:\n",
            "------------------------------------------------------------\n",
            "v1.0 (Production):\n",
            "  - Generic customer addressing\n",
            "  - No personalization\n",
            "  - Status: Approved ‚úÖ\n",
            "\n",
            "v1.1 (Testing):\n",
            "  - Personalized with customer name\n",
            "  - Policy-type aware\n",
            "  - Customer tenure aware\n",
            "  - Standardized closing\n",
            "  - Status: Testing üß™\n",
            "\n",
            "üìä Next step: A/B testing to compare quality\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# COMPONENT 2: Model Registry\n",
        "# ========================================\n",
        "\n",
        "print(\"ü§ñ Building Model Registry...\")\n",
        "print()\n",
        "\n",
        "# Define our approved model catalog\n",
        "model_catalog = {\n",
        "    \"gemini-1.5-pro\": {\n",
        "        \"model_id\": \"gemini-1.5-pro\",\n",
        "        \"display_name\": \"Gemini 1.5 Pro\",\n",
        "        \"provider\": \"Google\",\n",
        "        \"model_type\": \"foundation\",\n",
        "        \"status\": \"approved\",\n",
        "        \"tier\": \"premium\",\n",
        "        \"capabilities\": [\"text-generation\", \"code-generation\", \"analysis\"],\n",
        "        \"max_tokens\": 2000000,  # 2M token context window\n",
        "        \"approved_date\": \"2024-11-01\",\n",
        "        \"approved_by\": \"ML Governance Committee\"\n",
        "    },\n",
        "\n",
        "    \"gemini-1.5-flash\": {\n",
        "        \"model_id\": \"gemini-1.5-flash\",\n",
        "        \"display_name\": \"Gemini 1.5 Flash\",\n",
        "        \"provider\": \"Google\",\n",
        "        \"model_type\": \"foundation\",\n",
        "        \"status\": \"approved\",\n",
        "        \"tier\": \"standard\",\n",
        "        \"capabilities\": [\"text-generation\", \"high-volume-tasks\"],\n",
        "        \"max_tokens\": 1000000,  # 1M token context window\n",
        "        \"approved_date\": \"2024-11-01\",\n",
        "        \"approved_by\": \"ML Governance Committee\"\n",
        "    },\n",
        "\n",
        "    \"claude-3-opus\": {\n",
        "        \"model_id\": \"claude-3-opus\",\n",
        "        \"display_name\": \"Claude 3 Opus\",\n",
        "        \"provider\": \"Anthropic\",\n",
        "        \"model_type\": \"foundation\",\n",
        "        \"status\": \"approved\",\n",
        "        \"tier\": \"premium\",\n",
        "        \"capabilities\": [\"text-generation\", \"analysis\", \"long-context\"],\n",
        "        \"max_tokens\": 200000,  # 200K token context\n",
        "        \"approved_date\": \"2024-10-15\",\n",
        "        \"approved_by\": \"ML Governance Committee\"\n",
        "    },\n",
        "\n",
        "    \"customer-support-v1.2\": {\n",
        "        \"model_id\": \"customer-support-v1.2\",\n",
        "        \"display_name\": \"Customer Support Model v1.2\",\n",
        "        \"provider\": \"Prudential (Fine-tuned Gemini)\",\n",
        "        \"model_type\": \"fine-tuned\",\n",
        "        \"base_model\": \"gemini-1.5-pro\",\n",
        "        \"status\": \"approved\",\n",
        "        \"tier\": \"custom\",\n",
        "        \"capabilities\": [\"customer-service\", \"policy-questions\"],\n",
        "        \"max_tokens\": 2000000,\n",
        "        \"approved_date\": \"2024-11-15\",\n",
        "        \"approved_by\": \"ML Governance Committee\",\n",
        "        \"training_data\": \"gs://prudential-data/customer-support-conversations\",\n",
        "        \"use_case_restriction\": \"customer-support-only\"\n",
        "    },\n",
        "\n",
        "    \"gpt-4\": {\n",
        "        \"model_id\": \"gpt-4\",\n",
        "        \"display_name\": \"GPT-4\",\n",
        "        \"provider\": \"OpenAI\",\n",
        "        \"model_type\": \"foundation\",\n",
        "        \"status\": \"deprecated\",\n",
        "        \"tier\": \"premium\",\n",
        "        \"capabilities\": [\"text-generation\"],\n",
        "        \"max_tokens\": 128000,\n",
        "        \"deprecated_date\": \"2024-12-01\",\n",
        "        \"deprecated_reason\": \"Security review failed - data residency concerns\",\n",
        "        \"replacement_model\": \"gemini-1.5-pro\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save catalog to file\n",
        "catalog_file = '/content/genaiops/models/model_catalog.json'\n",
        "with open(catalog_file, 'w') as f:\n",
        "    json.dump(model_catalog, f, indent=2)\n",
        "\n",
        "print(f\"‚úÖ Model catalog created with {len(model_catalog)} models\")\n",
        "print()\n",
        "\n",
        "# Display summary\n",
        "print(\"üìä Model Summary:\")\n",
        "print(\"-\" * 60)\n",
        "for model_id, details in model_catalog.items():\n",
        "    status_emoji = \"‚úÖ\" if details[\"status\"] == \"approved\" else \"‚ö†Ô∏è\" if details[\"status\"] == \"testing\" else \"‚ùå\"\n",
        "    print(f\"{status_emoji} {details['display_name']}\")\n",
        "    print(f\"   Status: {details['status']} | Provider: {details['provider']} | Tier: {details['tier']}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMPTtkbcxRIy",
        "outputId": "95d389f5-c792-4ef5-96ee-15214b93a0f1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü§ñ Building Model Registry...\n",
            "\n",
            "‚úÖ Model catalog created with 5 models\n",
            "\n",
            "üìä Model Summary:\n",
            "------------------------------------------------------------\n",
            "‚úÖ Gemini 1.5 Pro\n",
            "   Status: approved | Provider: Google | Tier: premium\n",
            "\n",
            "‚úÖ Gemini 1.5 Flash\n",
            "   Status: approved | Provider: Google | Tier: standard\n",
            "\n",
            "‚úÖ Claude 3 Opus\n",
            "   Status: approved | Provider: Anthropic | Tier: premium\n",
            "\n",
            "‚úÖ Customer Support Model v1.2\n",
            "   Status: approved | Provider: Prudential (Fine-tuned Gemini) | Tier: custom\n",
            "\n",
            "‚ùå GPT-4\n",
            "   Status: deprecated | Provider: OpenAI | Tier: premium\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# Add Cost Information to Model Registry\n",
        "# ========================================\n",
        "\n",
        "print(\"üí∞ Adding cost tracking to Model Registry...\")\n",
        "print()\n",
        "\n",
        "# Cost data for each model (pricing per 1K tokens)\n",
        "model_costs = {\n",
        "    \"gemini-1.5-pro\": {\n",
        "        \"input_cost_per_1k\": 0.00125,   # $0.00125 per 1K input tokens\n",
        "        \"output_cost_per_1k\": 0.005,    # $0.005 per 1K output tokens\n",
        "        \"cost_tier\": \"premium\",\n",
        "        \"notes\": \"Best quality, highest cost\"\n",
        "    },\n",
        "\n",
        "    \"gemini-1.5-flash\": {\n",
        "        \"input_cost_per_1k\": 0.000075,  # $0.000075 per 1K input tokens\n",
        "        \"output_cost_per_1k\": 0.0003,   # $0.0003 per 1K output tokens\n",
        "        \"cost_tier\": \"budget\",\n",
        "        \"notes\": \"Fast and cheap, good for high-volume\"\n",
        "    },\n",
        "\n",
        "    \"claude-3-opus\": {\n",
        "        \"input_cost_per_1k\": 0.015,     # $0.015 per 1K input tokens\n",
        "        \"output_cost_per_1k\": 0.075,    # $0.075 per 1K output tokens\n",
        "        \"cost_tier\": \"premium-plus\",\n",
        "        \"notes\": \"Most expensive, best for complex tasks\"\n",
        "    },\n",
        "\n",
        "    \"customer-support-v1.2\": {\n",
        "        \"input_cost_per_1k\": 0.00125,   # Same as base Gemini Pro\n",
        "        \"output_cost_per_1k\": 0.005,\n",
        "        \"cost_tier\": \"premium\",\n",
        "        \"training_cost\": 850.00,        # One-time training cost\n",
        "        \"notes\": \"Fine-tuned model, training cost already paid\"\n",
        "    },\n",
        "\n",
        "    \"gpt-4\": {\n",
        "        \"input_cost_per_1k\": 0.03,      # Expensive (deprecated)\n",
        "        \"output_cost_per_1k\": 0.06,\n",
        "        \"cost_tier\": \"deprecated\",\n",
        "        \"notes\": \"Deprecated - do not use\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save cost data\n",
        "cost_file = '/content/genaiops/models/model_costs.json'\n",
        "with open(cost_file, 'w') as f:\n",
        "    json.dump(model_costs, f, indent=2)\n",
        "\n",
        "print(\"‚úÖ Cost tracking added for all models\")\n",
        "print()\n",
        "\n",
        "# Display cost comparison\n",
        "print(\"üíµ Cost Comparison (per 1K tokens):\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"{'Model':<30} {'Input Cost':>12} {'Output Cost':>12} {'Tier':<15}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for model_id, costs in model_costs.items():\n",
        "    if model_id in model_catalog and model_catalog[model_id][\"status\"] != \"deprecated\":\n",
        "        input_cost = f\"${costs['input_cost_per_1k']:.6f}\"\n",
        "        output_cost = f\"${costs['output_cost_per_1k']:.6f}\"\n",
        "        tier = costs['cost_tier']\n",
        "\n",
        "        model_name = model_catalog[model_id][\"display_name\"]\n",
        "        print(f\"{model_name:<30} {input_cost:>12} {output_cost:>12} {tier:<15}\")\n",
        "\n",
        "print()\n",
        "\n",
        "# Calculate example: 1 million tokens\n",
        "print(\"üìä Example Cost Calculation:\")\n",
        "print(\"Scenario: Process 1M input tokens + generate 500K output tokens\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "example_input_tokens = 1000000  # 1M tokens\n",
        "example_output_tokens = 500000  # 500K tokens\n",
        "\n",
        "for model_id in [\"gemini-1.5-pro\", \"gemini-1.5-flash\", \"claude-3-opus\"]:\n",
        "    if model_id in model_costs:\n",
        "        costs = model_costs[model_id]\n",
        "\n",
        "        # Calculate cost\n",
        "        input_cost = (example_input_tokens / 1000) * costs['input_cost_per_1k']\n",
        "        output_cost = (example_output_tokens / 1000) * costs['output_cost_per_1k']\n",
        "        total_cost = input_cost + output_cost\n",
        "\n",
        "        model_name = model_catalog[model_id][\"display_name\"]\n",
        "        print(f\"{model_name:<30} Total Cost: ${total_cost:,.2f}\")\n",
        "\n",
        "print()\n",
        "print(\"üí° Insight: Gemini Flash is 94% cheaper than Claude Opus for high-volume tasks!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVg_Xcw4IiMI",
        "outputId": "b95145d3-bc02-435b-9221-c660e152ae60"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üí∞ Adding cost tracking to Model Registry...\n",
            "\n",
            "‚úÖ Cost tracking added for all models\n",
            "\n",
            "üíµ Cost Comparison (per 1K tokens):\n",
            "--------------------------------------------------------------------------------\n",
            "Model                            Input Cost  Output Cost Tier           \n",
            "--------------------------------------------------------------------------------\n",
            "Gemini 1.5 Pro                    $0.001250    $0.005000 premium        \n",
            "Gemini 1.5 Flash                  $0.000075    $0.000300 budget         \n",
            "Claude 3 Opus                     $0.015000    $0.075000 premium-plus   \n",
            "Customer Support Model v1.2       $0.001250    $0.005000 premium        \n",
            "\n",
            "üìä Example Cost Calculation:\n",
            "Scenario: Process 1M input tokens + generate 500K output tokens\n",
            "--------------------------------------------------------------------------------\n",
            "Gemini 1.5 Pro                 Total Cost: $3.75\n",
            "Gemini 1.5 Flash               Total Cost: $0.22\n",
            "Claude 3 Opus                  Total Cost: $52.50\n",
            "\n",
            "üí° Insight: Gemini Flash is 94% cheaper than Claude Opus for high-volume tasks!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gpant5VtNj6D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}